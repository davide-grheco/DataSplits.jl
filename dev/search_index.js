var documenterSearchIndex = {"docs":
[{"location":"07-kennard-stone/#Kennard–Stone-Split","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"The Kennard–Stone algorithm (also known as CADEX in some literature) is a deterministic method for selecting a representative training set from a dataset. It iteratively chooses the sample that is farthest (in feature space) from all previously selected samples, starting from the most distant pair. This ensures that the training set covers the full range of the feature space, making it especially useful for rational dataset splitting.","category":"page"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"CADEX stands for Computer Aided Design of Experiments and is an alias for the Kennard–Stone algorithm in DataSplits.","category":"page"},{"location":"07-kennard-stone/#How-it-works","page":"Kennard–Stone Split","title":"How it works","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"Compute the pairwise distance matrix for all samples.\nSelect the two samples that are farthest apart as the initial training set.\nIteratively add the sample that is farthest from the current training set (i.e., has the largest minimum distance to any selected sample).\nContinue until the desired number of training samples is reached.","category":"page"},{"location":"07-kennard-stone/#Usage","page":"Kennard–Stone Split","title":"Usage","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"using DataSplits, Distances\nsplitter = KennardStoneSplit(0.8)\ntrain, test = split(X, splitter)","category":"page"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"X: Data matrix (samples × features)\n0.8: Fraction of samples to use for training","category":"page"},{"location":"07-kennard-stone/#Options","page":"Kennard–Stone Split","title":"Options","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"KennardStoneSplit(frac; metric=Euclidean()): frac is the fraction of samples to use for training (between 0 and 1). metric is the distance metric (default: Euclidean).","category":"page"},{"location":"07-kennard-stone/#Implementation-Note","page":"Kennard–Stone Split","title":"Implementation Note","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"For very large datasets, use LazyKennardStone which is a memory-efficient variant of this algorithm.","category":"page"},{"location":"07-kennard-stone/#See-also","page":"Kennard–Stone Split","title":"See also","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"SPXY Split: Jointly maximizes diversity in both features and target property.","category":"page"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"","category":"page"},{"location":"07-kennard-stone/#Reference","page":"Kennard–Stone Split","title":"Reference","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"Kennard, R. W.; Stone, L. A. Computer Aided Design of Experiments. Technometrics 1969, 11 (1), 137–148. https://doi.org/10.1080/00401706.1969.10490666.","category":"page"},{"location":"09-spxy/#SPXY-Split","page":"SPXY Split","title":"SPXY Split","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"The SPXY algorithm extends Kennard–Stone by considering both the feature matrix (X) and the target vector (y) when splitting data. It constructs a joint distance matrix as the sum of normalized pairwise distances in X and y, then applies the maximin selection. This ensures the training set is diverse in both predictors and response, which is especially important for regression tasks where the target distribution matters.","category":"page"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"MDKSSplit(frac): Alias for SPXYSplit(frac; metric=Mahalanobis()) (SPXY algorithm using Mahalanobis distance).","category":"page"},{"location":"09-spxy/#How-it-works","page":"SPXY Split","title":"How it works","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Compute the normalized pairwise distance matrix for X (features).\nCompute the normalized pairwise distance matrix for y (target).\nAdd the two matrices to form a joint distance matrix.\nApply the Kennard–Stone maximin selection on the joint distance matrix to select a representative training set.","category":"page"},{"location":"09-spxy/#Usage","page":"SPXY Split","title":"Usage","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"using DataSplits, Distances\ntrain, test = split(X, y, SPXYSplit(0.7; metric=Cityblock()))","category":"page"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"X: Feature matrix (samples × features)\ny: Target vector (length matches number of samples)\n0.7: Fraction of samples to use for training\nmetric: Distance metric for both X and y (default: Euclidean)","category":"page"},{"location":"09-spxy/#Options","page":"SPXY Split","title":"Options","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"SPXYSplit(frac; metric=Euclidean()): frac is the fraction of samples to use for training (between 0 and 1). metric is the distance metric (default: Euclidean).\nMDKSSplit(frac): Alias for SPXY with Mahalanobis distance.","category":"page"},{"location":"09-spxy/#Notes","page":"SPXY Split","title":"Notes","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"This algorithm is most appropriate for regression and continuous targets.\nFor classification, you may need to encode the target appropriately.\nYou must call split((X, y), strategy) or split(X, y, strategy); calling split(X, strategy) will error.","category":"page"},{"location":"09-spxy/#See-also","page":"SPXY Split","title":"See also","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Kennard–Stone Split","category":"page"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"","category":"page"},{"location":"09-spxy/#Reference","page":"SPXY Split","title":"Reference","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Galvao, R.; Araujo, M.; Jose, G.; Pontes, M.; Silva, E.; Saldanha, T. A Method for Calibration and Validation Subset Partitioning. Talanta 2005, 67 (4), 736–740. https://doi.org/10.1016/j.talanta.2005.03.025. Saptoro, A.; Tadé, M. O.; Vuthaluru, H. A Modified Kennard-Stone Algorithm for Optimal Division of Data for Developing Artificial Neural Network Models. Chemical Product and Process Modeling 2012, 7 (1). https://doi.org/10.1515/1934-2659.1645.","category":"page"},{"location":"06-examples-tutorials/#06.-Examples-and-Tutorials","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"","category":"section"},{"location":"06-examples-tutorials/#Example:-SPXY-Split-on-Custom-Data","page":"06. Examples & Tutorials","title":"Example: SPXY Split on Custom Data","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"using DataSplits, Distances\n# Suppose `df` is a DataFrame with features and target\ndf = DataFrame(rand(100, 5), :auto)\ny = rand(100)\ntrain, test = split((Matrix(df), y), SPXYSplit(0.75))","category":"page"},{"location":"06-examples-tutorials/#Example:-Custom-Splitter-Implementation","page":"06. Examples & Tutorials","title":"Example: Custom Splitter Implementation","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"Suppose you want to create a splitter that always assigns the first 80% of samples to train:","category":"page"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"struct First80Split <: SplitStrategy end\n\nfunction first80(N, s, rng, data)\n    cut = floor(Int, 0.8 * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, ::First80Split; rng=nothing)\n    split_with_positions(data, nothing, first80; rng=rng)\nend","category":"page"},{"location":"06-examples-tutorials/#Example:-Group-aware-Splitting","page":"06. Examples & Tutorials","title":"Example: Group-aware Splitting","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"using DataSplits\n# Suppose you have cluster assignments for your data\nclusters = [rand(1:5) for _ in 1:100]\nusing Clustering\nres = Clustering.ClusteringResult(clusters)\ntrain, test = split(X, ClusterShuffleSplit(res, 0.7))","category":"page"},{"location":"10-optisim/#OptiSim-Split","page":"OptiSim Split","title":"OptiSim Split","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"OptiSim is an iterative, optimisable splitting algorithm that aims to maximize the diversity of the training set by minimizing within-set similarity. At each step, it generates a temporary subsample of candidates and selects the one most dissimilar to the current training set, repeating until the desired fraction is reached. The algorithm is flexible and can be tuned for speed or quality by adjusting the subsample size and distance cutoff.","category":"page"},{"location":"10-optisim/#How-it-works","page":"OptiSim Split","title":"How it works","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"Compute the pairwise distance matrix for all samples.\nStart with a random sample in the training set.\nAt each iteration, generate a candidate subsample.\nSelect the candidate most dissimilar to the current training set.\nRepeat until the desired number of training samples is reached.","category":"page"},{"location":"10-optisim/#Usage","page":"OptiSim Split","title":"Usage","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"using DataSplits\ntrain, test = split(X, OptiSimSplit(0.8; max_subsample_size=50))","category":"page"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"X: Data matrix (samples × features)\n0.8: Fraction of samples to use for training\nmax_subsample_size: Size of candidate pool at each step (default: 0, i.e., use all)\ndistance_cutoff: Threshold for similarity filtering (default: 0.35)\nmetric: Distance metric (default: Euclidean)","category":"page"},{"location":"10-optisim/#Options","page":"OptiSim Split","title":"Options","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"OptiSimSplit(frac; max_subsample_size=0, distance_cutoff=0.35, metric=Euclidean())","category":"page"},{"location":"10-optisim/#Notes","page":"OptiSim Split","title":"Notes","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"This algorithm depends on the first selection which is random. Different runs may result in very different compositions of training and test set.\nMinimumDissimilaritySplit is an alias for OptiSimSplit with max_subsample_size=1. This provides a fast, greedy variant for large datasets where speed is more important than optimal diversity.\nMaximumDissimilaritySplit is an alias for OptiSimSplit with max_subsample_size=N. This yields a greedy, diversity-maximizing split. Note: This implementation does not discard the first two samples as in the original Maximum Dissimilarity algorithm, but otherwise follows the same greedy logic. This algorithm is known to greedily include outliers.","category":"page"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"","category":"page"},{"location":"10-optisim/#Reference","page":"OptiSim Split","title":"Reference","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"Clark, R. D. OptiSim:  An Extended Dissimilarity Selection Method for Finding Diverse Representative Subsets. J. Chem. Inf. Comput. Sci. 1997, 37 (6), 1181–1188. https://doi.org/10.1021/ci970282v.","category":"page"},{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#DataSplits.ClusterShuffleSplit","page":"Reference","title":"DataSplits.ClusterShuffleSplit","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data; rng)\n\nGroup-aware train/test splitter: accepts either:\n\nA precomputed ClusteringResult.\nA clustering function f(data) that returns one.\n\nAt construction, clustering is executed so the strategy always holds a ClusteringResult.\n\nArguments:\n\nres or `f(...)\nfrac: fraction of samples in the training set (0 < frac < 1).\n\nThis splitter shuffles cluster IDs and accumulates whole clusters until frac * N samples are in the train set, then returns (train_idx, test_idx).\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.ClusterStratifiedSplit","page":"Reference","title":"DataSplits.ClusterStratifiedSplit","text":"ClusterStratifiedSplit(res::ClusteringResult, allocation::Symbol; n=nothing, frac)\nClusterStratifiedSplit(f::Function, allocation::Symbol; n=nothing, frac, data)\n\nCluster-stratified train/test splitter.\n\nSplits each cluster into train/test using one of three allocation methods:\n\n:equal: Randomly selects n samples from each cluster, then splits them into train/test according to frac.\n:proportional: Uses all samples in each cluster, splits them into train/test according to frac.\n:neyman: Randomly selects a Neyman quota from each cluster (based on pooled std), then splits into train/test according to frac.\n\nArguments\n\nres or f(...): ClusteringResult or clustering function.\nallocation: :equal, :proportional, or :neyman.\nn: Number of samples per cluster (equal/neyman allocation).\nfrac: Fraction of selected samples to use for train (rest go to test).\n\nNotes\n\nIf n is greater than the cluster size, all samples in the cluster are used.\nFor :proportional, all samples are always used.\nFor frac=1.0, all selected samples go to train; for frac=0.0, all go to test.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.KennardStoneSplit","page":"Reference","title":"DataSplits.KennardStoneSplit","text":"KennardStoneSplit{T} <: SplitStrategy\n\nA splitting strategy implementing the Kennard-Stone algorithm for train/test splitting.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric to use (default: Euclidean())\n\nExamples\n\n```julia\n\nCreate a splitter with 80% training data using Euclidean distance\n\nsplitter = KennardStoneSplit(0.8)\n\nCreate a splitter with custom metric\n\nusing Distances splitter = KennardStoneSplit(0.7, Cityblock())\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.OptiSimSplit","page":"Reference","title":"DataSplits.OptiSimSplit","text":"OptiSimSplit(frac; n_clusters = 10, max_subsample_size, distance_cutoff = 0.10,\n             metric = Euclidean(), random_state = 42)\n\nImplementation of OptiSim (Clark 1998, J. Chem. Inf. Comput. Sci.), an optimisable K‑dissimilarity selection.\n\nfrac – fraction of samples to return in the training subset\nn_clusters = M – requested cluster/selection‑set size\nmax_subsample_size = K – size of the temporary sub‑sample (default: max(1, ceil(Int, 0.05N)))\ndistance_cutoff = c – two points are “similar” if their distance < c\nmetric – any Distances.jl metric\nrandom_state – seed for the RNG\n\nThe splitter requires both an X matrix and target vector y when calling split.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SPXYSplit-Tuple{Real}","page":"Reference","title":"DataSplits.SPXYSplit","text":"SPXYSplit(frac; metric = Euclidean())\n\nCreate an SPXY splitter – the variant of Kennard–Stone in which the distance matrix is the element‑wise sum of\n\nthe (normalised) pairwise distance matrix of the feature matrix X\nplus the (normalised) pairwise distance matrix of the response vector y.\n\nfrac is the fraction of samples that will end up in the training subset.\n\nnote: Note\nsplit must be called with a 2‑tuple (X, y) or with positional arguments split(X, y, strategy); calling split(X, strategy) will raise a MethodError, because y is mandatory for SPXY.\n\nArguments\n\nname type meaning\nfrac Real (0 < frac < 1) training‑set fraction\nmetric [Distances.SemiMetric] distance metric used for both X and y\n\nSee also\n\nKennardStoneSplit — the classical variant that uses only X.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.SphereExclusionResult","page":"Reference","title":"DataSplits.SphereExclusionResult","text":"SphereExclusionResult\n\nResult of sphere exclusion clustering.\n\nFields:\n\nassignments::Vector{Int}: cluster index per point.\nradius::Float64: exclusion radius.\nmetric::Distances.SemiMetric: distance metric.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TargetPropertySplit","page":"Reference","title":"DataSplits.TargetPropertySplit","text":"TargetPropertySplit{T} <: SplitStrategy\n\nA splitting strategy that partitions a 1D property array into train/test sets by sorting the property values.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc, :desc, :high, :low, :largest, :smallest, etc.\n\nExamples\n\nsplit(y, TargetPropertyHigh(0.8))\nsplit(X[:, 3], TargetPropertyLow(0.5))\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TimeSplit","page":"Reference","title":"DataSplits.TimeSplit","text":"TimeSplit{T} <: SplitStrategy\n\nSplits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc (oldest in train, default), :desc (newest in train)\n\nExamples\n\nsplit(dates, TimeSplitOldest(0.7))\nsplit(dates, TimeSplitNewest(0.3))\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits._split-Tuple{AbstractArray, AbstractVector, SPXYSplit}","page":"Reference","title":"DataSplits._split","text":"_split(X, y, strategy::SPXYSplit; rng = Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nSplit a feature matrix X and response vector y into train/test subsets using the SPXY algorithm:\n\nBuild the joint distance matrix D = D_X + D_Y  (see SPXYSplit for details).\nRun the Kennard–Stone maximin procedure on D.\nReturn two sorted index vectors (train_idx, test_idx).\n\nArguments\n\nname type requirement\nX AbstractMatrix size(X, 1) == length(y)\ny AbstractVector \nstrategy SPXYSplit created with SPXYSplit(frac; metric)\nrng random‑number source unused here but kept for API symmetry\n\nReturns\n\nTwo Vector{Int} with the row indices of X (and the corresponding entries of y) that belong to the training and test subsets.\n\nThe indices are axis‑correct — if X is an OffsetMatrix whose first row is index 0, the returned indices will also start at 0.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.cluster_stratified-NTuple{4, Any}","page":"Reference","title":"DataSplits.cluster_stratified","text":"cluster_stratified(N, s, rng, data)\n\nMain splitting function. For each cluster, selects indices according to the allocation method, then splits those indices into train/test according to frac.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.distance_matrix-Tuple{Any, Distances.PreMetric}","page":"Reference","title":"DataSplits.distance_matrix","text":"distance_matrix(X, metric::PreMetric)\n\nCompute the full symmetric pairwise distance matrix D for the dataset X using the given metric. This function uses get_sample to access samples, ensuring compatibility with any custom array type that implements get_sample and sample_indices.\n\nReturns a matrix D such that D[i, j] = metric(xᵢ, xⱼ) and D[i, j] == D[j, i].\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.equal_allocation-NTuple{4, Any}","page":"Reference","title":"DataSplits.equal_allocation","text":"equal_allocation(cl_ids, idxs_by_cluster, n, rng)\n\nRandomly select n samples from each cluster (or all if cluster is smaller). Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_maximin_element-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Union{AbstractSet{Int64}, AbstractVector{Int64}}, Union{AbstractSet{Int64}, AbstractVector{Int64}}}} where T<:Real","page":"Reference","title":"DataSplits.find_maximin_element","text":"find_maximin_element(distances::AbstractMatrix{T},\n                    source_set::AbstractVector{Int},\n                    reference_set::AbstractVector{Int}) -> Int\n\nFind the element in source_set that maximizes the minimum distance to all elements in reference_set.\n\nArguments\n\ndistances::AbstractMatrix{T}: Precomputed, symmetric pairwise distance matrix.\nsource_set::AbstractVector{Int}: Set of items to evaluate.\nreference_set::AbstractVector{Int}: Set of items to compare against.\n\nReturns\n\nInt: The index in source_set that is farthest from its nearest neighbour in reference_set.\n\nNotes\n\nIf reference_set is empty, throws ArgumentError\nBreaks ties by returning the first maximum\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_most_distant_pair-Tuple{AbstractMatrix}","page":"Reference","title":"DataSplits.find_most_distant_pair","text":"find_most_distant_pair(D::AbstractMatrix) → (i, j)\n\nFinds indices of most distant pair in precomputed distance matrix.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.get_sample-Tuple{AbstractArray, Any}","page":"Reference","title":"DataSplits.get_sample","text":"get_sample(A::AbstractArray, idx)\n\nPublic API for getting samples that handles any valid index type. Dispatches to internalget_sample after index conversion.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.is_sample_array-Tuple{Type}","page":"Reference","title":"DataSplits.is_sample_array","text":"is_sample_array(::Type{T}) -> Bool\n\nTrait method that tells whether type T behaves like a sample‑indexed array that supports sample_indices, _get_sample, etc.\n\nYou may extend this for custom containers to ensure compatibility with OptiSim or other sampling methods.\n\nDefaults to false unless specialized.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.kennardstone-NTuple{4, Any}","page":"Reference","title":"DataSplits.kennardstone","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nOptimized in-memory Kennard-Stone algorithm using precomputed distance matrix. Best for small-to-medium datasets where O(N²) memory is acceptable.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.lazy_kennard_stone-NTuple{4, Any}","page":"Reference","title":"DataSplits.lazy_kennard_stone","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nKennard-Stone (CADEX) algorithm for optimal train/test splitting using maximin strategy. Memory-optimized implementation with O(N) storage. Useful when working with large datasets where the NxN distance matrix does not fit memory. When working with small datasets, use the traditional implementation.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.neyman_allocation-NTuple{5, Any}","page":"Reference","title":"DataSplits.neyman_allocation","text":"neyman_allocation(cl_ids, idxs_by_cluster, n, data, rng)\n\nRandomly select a Neyman quota from each cluster (proportional to cluster size and mean std of features). Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.proportional_allocation-Tuple{Any, Any, Any}","page":"Reference","title":"DataSplits.proportional_allocation","text":"proportional_allocation(cl_ids, idxs_by_cluster, rng)\n\nUse all samples in each cluster, shuffled. Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sample_indices-Tuple{AbstractArray}","page":"Reference","title":"DataSplits.sample_indices","text":"sample_indices(A::AbstractArray) -> AbstractVector\n\nReturn a vector of valid sample indices for A`, supporting all AbstractArray types.\n\nThis method defines the \"sample axis\" (typically axis 1) and determines how your splitting/sampling algorithms enumerate data points.\n\nDefault\n\nFor standard arrays, returns axes(A, 1).\n\nExtension\n\nTo support non-standard arrays (e.g., views, custom wrappers), you may extend this method to expose logical sample indices:\n\nBase.sample_indices(a::MyFancyArray) = 1:length(a.ids)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sphere_exclusion-Tuple{Any}","page":"Reference","title":"DataSplits.sphere_exclusion","text":"sphere_exclusion(data; radius::Real, metric::Distances.SemiMetric=Euclidean()) -> SphereExclusionResult\n\nCluster samples in data by sphere exclusion:\n\nCompute full pairwise distance matrix and normalize values to [0,1].\nWhile unassigned samples remain:\nPick first unassigned sample i.\nAll unassigned samples j with normalized distance D[i,j] <= radius form a cluster.\nMark them assigned and increment cluster ID.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split-Tuple{Any, SplitStrategy}","page":"Reference","title":"DataSplits.split","text":"split(data, strategy; rng=Random.default_rng()) → (train, test)\n\nSplit data into train/test sets according to strategy.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split_with_positions-Tuple{Any, Any, Any}","page":"Reference","title":"DataSplits.split_with_positions","text":"split_with_positions(data, s, core_algorithm; rng=Random.default_rng(), args...)\n\nGeneric wrapper for split strategies. Handles mapping between user indices and 1:N positions.\n\ndata: The user’s data array.\ns: The split strategy object.\ncore_algorithm: Function (N, s, rng, data, args...) -> (trainpos, testpos)\n\nReturns: (trainidx, testidx) as indices valid for data.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.targetpropertysplit-Tuple{Any, Any, Any, AbstractVector}","page":"Reference","title":"DataSplits.targetpropertysplit","text":"targetpropertysplit(N, s, rng, data)\n\nSorts the property array and splits into train/test according to s.frac and s.order.\n\n\n\n\n\n","category":"method"},{"location":"03-core-api-reference/#03.-Core-API-Reference","page":"03. Core API Reference","title":"03. Core API Reference","text":"","category":"section"},{"location":"03-core-api-reference/#split","page":"03. Core API Reference","title":"split","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"split(data, strategy; rng)","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"Dispatches to strategy._split. Returns (train, test) index vectors.","category":"page"},{"location":"03-core-api-reference/#split*with*positions","page":"03. Core API Reference","title":"splitwithpositions","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"split_with_positions(data, s, core_algorithm; rng=Random.default_rng())","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"A utility function that handles mapping between user-facing indices (which may be non-1-based or non-contiguous) and internal 1:N positions. The core_algorithm should operate on positions 1:N and return (train_pos, test_pos), which are then mapped back to the correct indices for the user.","category":"page"},{"location":"03-core-api-reference/#SplitStrategy-Interface","page":"03. Core API Reference","title":"SplitStrategy Interface","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"To add a new strategy, subtype SplitStrategy and implement:","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"_split(data, s::YourStrategy; rng)","category":"page"},{"location":"03-core-api-reference/#Utility-Functions","page":"03. Core API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"sample_indices(data): returns iterable indices of samples (default: 1:size(data,1), but now robust to any AbstractArray axes).\nValidFraction: bounds-checked fraction type for split ratios.","category":"page"},{"location":"03-core-api-reference/#Example:-Custom-Splitter-(new-pattern)","page":"03. Core API Reference","title":"Example: Custom Splitter (new pattern)","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    split_with_positions(data, s, mysplit; rng=rng)\nend","category":"page"},{"location":"02-getting-started/#02.-Getting-Started","page":"02. Getting Started","title":"02. Getting Started","text":"","category":"section"},{"location":"02-getting-started/#Basic-API","page":"02. Getting Started","title":"Basic API","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"split(data, strategy): main entry point.\ndata: array, tuple (X,) or (X, y).\nstrategy: subtype of SplitStrategy.","category":"page"},{"location":"02-getting-started/#Data-Formats","page":"02. Getting Started","title":"Data Formats","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"Accepts matrices, tables, or custom types implementing sample_indices.","category":"page"},{"location":"02-getting-started/#Robust-Index-Handling","page":"02. Getting Started","title":"Robust Index Handling","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"All splitting strategies in DataSplits are robust to arrays with arbitrary axes (e.g., OffsetArrays, SubArrays, etc.). The library automatically handles mapping between user-facing indices and internal positions, so you can use any AbstractArray as input.","category":"page"},{"location":"02-getting-started/#Randomness-Control","page":"02. Getting Started","title":"Randomness Control","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"Pass rng keyword to strategies supporting it, e.g. split(X, RandomSplit(0.7); rng=123).","category":"page"},{"location":"02-getting-started/#Example:-Custom-Data-Type","page":"02. Getting Started","title":"Example: Custom Data Type","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"To use your own data type, implement sample_indices(data) and get_sample(data, i).","category":"page"},{"location":"13-sphere-exclusion/#13.-Sphere-Exclusion-Split","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Constructor:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"SphereExclusionSplit(frac::Real; radius::Real, metric=Euclidean())","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Description: Sphere Exclusion Split is a clustering-based splitting strategy. It works by iteratively picking an unassigned sample and forming a cluster of all points within a specified radius (in normalized distance). This process repeats until all samples are assigned to clusters. Clusters are then assigned to the training set until the desired fraction is reached. This method is especially useful for spatial or similarity-based data, where you want to avoid splitting local neighborhoods.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When to use:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"For spatial, chemical, or biological data where locality matters.\nWhen you want to avoid splitting similar samples across train/test.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When not to use:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When you do not know how to set a meaningful radius.\nFor data with no meaningful distance metric.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Arguments:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"frac: training fraction.\nradius: normalized distance threshold [0,1].\nmetric: distance metric.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Usage:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"using DataSplits\ntrain, test = split(X, SphereExclusionSplit(0.7; radius=0.2))","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Pros:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Intuitive spatial grouping.\nAvoids splitting local neighborhoods.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Cons:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Sensitive to radius; cluster sizes may be uneven.\nMay not work well for high-dimensional or non-metric data.","category":"page"},{"location":"05-extending-data-splits/#05.-Extending-DataSplits","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"","category":"section"},{"location":"05-extending-data-splits/#Custom-Clustering","page":"05. Extending DataSplits","title":"Custom Clustering","text":"","category":"section"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"To add a new clustering algorithm:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Subtype ClusteringResult.\nImplement assignments, nclusters, counts, wcounts for your result type.\nRegister a clustering function returning your result.","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Example:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"struct MyClusteringResult <: ClusteringResult\n  assignments::Vector{Int}\nend\nassignments(r::MyClusteringResult) = r.assignments\nnclusters(r::MyClusteringResult) = maximum(r.assignments)\n# ...","category":"page"},{"location":"05-extending-data-splits/#Custom-Splitter","page":"05. Extending DataSplits","title":"Custom Splitter","text":"","category":"section"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"To add a new splitting strategy:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Subtype SplitStrategy.\nImplement a core function (e.g., mysplit(N, s, rng, data)) that returns (train_pos, test_pos) for positions 1:N.\nImplement _split(data, s; rng) to call split_with_positions(data, s, mysplit; rng=rng).\nUse ValidFraction for fraction validation.","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Example:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    split_with_positions(data, s, mysplit; rng=rng)\nend","category":"page"},{"location":"14-cluster-stratified/#Cluster-Stratified-Split","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"Cluster Stratified Split is a train/test splitting strategy that ensures each cluster (as determined by a clustering algorithm) is split into train and test sets according to a specified allocation method and user-defined train fraction.","category":"page"},{"location":"14-cluster-stratified/#Allocation-Methods","page":"Cluster Stratified Split","title":"Allocation Methods","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"Equal allocation: Randomly selects a fixed number n of samples from each cluster, then splits those into train/test according to the user-specified frac (fraction for train set). The rest of the cluster is unused.\nProportional allocation: Uses all samples in each cluster, splits them into train/test according to the user-specified frac (fraction for train set).\nNeyman allocation: Randomly selects a quota from each cluster (proportional to cluster size and mean standard deviation of features), then splits those into train/test according to the user-specified frac (fraction for train set). The rest of the cluster is unused.","category":"page"},{"location":"14-cluster-stratified/#Usage","page":"Cluster Stratified Split","title":"Usage","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"using DataSplits, Clustering\n\n# Assume you have a ClusteringResult `clusters` and a data matrix X\nsplitter = ClusterStratifiedSplit(clusters, :equal; n=4, frac=0.7)\ntrain_idx, test_idx = split(X, splitter)\n\nsplitter = ClusterStratifiedSplit(clusters, :proportional; frac=0.7)\ntrain_idx, test_idx = split(X, splitter)\n\nsplitter = ClusterStratifiedSplit(clusters, :neyman; n=4, frac=0.7)\ntrain_idx, test_idx = split(X, splitter)","category":"page"},{"location":"14-cluster-stratified/#Arguments","page":"Cluster Stratified Split","title":"Arguments","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"clusters: ClusteringResult (from Clustering.jl)\nallocation: :equal, :proportional, or :neyman\nn: Number of samples per cluster (for :equal and :neyman)\nfrac: Fraction of selected samples to use for train (rest go to test)","category":"page"},{"location":"14-cluster-stratified/#Notes","page":"Cluster Stratified Split","title":"Notes","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"If n is greater than the cluster size, all samples in the cluster are used.\nFor :proportional, all samples are always used.\nFor frac=1.0, all selected samples go to train; for frac=0.0, all go to test.\nThe split is performed per cluster; rounding is handled so that the train set always gets the larger share when the split is not exact.","category":"page"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"See also: Clustering.jl","category":"page"},{"location":"01-introduction/#01.-Introduction","page":"01. Introduction","title":"01. Introduction","text":"","category":"section"},{"location":"01-introduction/#Motivation-and-Features","page":"01. Introduction","title":"Motivation and Features","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"DataSplits provides rational train/test splitting algorithms and clustering-based pre-processing for reproducible model evaluation. Unlike random splits, these methods ensure that the training and test sets are representative and diverse, which is especially important for small or structured datasets.","category":"page"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"Key Features:","category":"page"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"Multiple splitting strategies: maximin, clustering-based, group-aware, and more\nExtensible API for custom strategies\nWorks with arrays, tuples, and custom data types","category":"page"},{"location":"01-introduction/#Installation","page":"01. Introduction","title":"Installation","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"] add https://github.com/davide-grheco/DataSplits.jl","category":"page"},{"location":"01-introduction/#When-to-Use-DataSplits","page":"01. Introduction","title":"When to Use DataSplits","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"When random splits are not enough (e.g., small datasets, strong structure)\nWhen you need reproducible, rational splits for benchmarking\nWhen you want to preserve group or cluster structure in your splits","category":"page"},{"location":"01-introduction/#Quickstart-Example","page":"01. Introduction","title":"Quickstart Example","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"using DataSplits, Distances\n\n# Simple Kennard–Stone split\ntrain, test = split(X, KennardStoneSplit(0.8))\n\n# SPXY split on features and target\ntrain, test = split((X, y), SPXYSplit(0.7; metric=Cityblock()))","category":"page"},{"location":"14-cluster-shuffle/#14.-Cluster-Shuffle-Split","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Constructor:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data)","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Description: Cluster Shuffle Split is a group-aware splitting strategy. It either takes a precomputed clustering result or a function to generate one, then shuffles the cluster labels and accumulates whole clusters into the training set until the desired fraction is reached. This approach is ideal for grouped or clustered data where splitting within groups would break structure or introduce leakage.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When to use:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When your data has natural groups or clusters (e.g., patients, molecules, batches).\nWhen you want to avoid splitting groups across train/test.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When not to use:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When clusters are very imbalanced in size (fraction control is coarse).\nFor unstructured data with no meaningful groups.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Arguments:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"res or f,data: clustering result or function.\nfrac: training fraction.\nrng: optional RNG.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Usage:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"using DataSplits, Clustering\nres = sphere_exclusion(X; radius=0.3)\ntrain, test = split(X, ClusterShuffleSplit(res, 0.8))","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Pros:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Preserves group integrity.\nEasy to randomize splits.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Cons:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Fraction control is coarse; may overshoot/undershoot target.\nCluster sizes may vary widely.","category":"page"},{"location":"04-algorithms-overview/#04.-Algorithms-Overview","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Each splitter returns two sets of indices (train, test), partitioning samples according to the chosen strategy. Choose based on dataset size, desired diversity, or grouping needs.","category":"page"},{"location":"04-algorithms-overview/#Kennard–Stone-Split","page":"04. Algorithms Overview","title":"Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Iteratively selects samples by choosing the point farthest from all previously chosen points in the feature space. This approach ensures the training set uniformly covers the distribution of predictors.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Ideal for small to medium datasets where uniform feature coverage improves model generalization.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Deterministic selection ensures reproducible splits.\nProvides diverse representation of feature space.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Requires storing and computing full distance matrix, which can be memory-intensive.","category":"page"},{"location":"04-algorithms-overview/#Lazy-Kennard–Stone-Split","page":"04. Algorithms Overview","title":"Lazy Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: A streaming variant of Kennard–Stone that computes distances on the fly, avoiding the full matrix. Suitable when memory is constrained.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Large datasets where memory is a bottleneck.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Reduced memory overhead.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"May incur extra computational overhead due to repeated distance computations.","category":"page"},{"location":"04-algorithms-overview/#SPXY-Split","page":"04. Algorithms Overview","title":"SPXY Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Extends Kennard–Stone to both features (X) and target variable (y), selecting samples that maximize combined spread. Ensures training set represents both predictors and response.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Regression tasks where preserving target distribution is critical.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Balances feature and response diversity.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Still requires pairwise distance computations; may overweight target variation.","category":"page"},{"location":"04-algorithms-overview/#OptiSim-Split","page":"04. Algorithms Overview","title":"OptiSim Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Applies iterative swapping of samples between train and test sets to minimize within-set dissimilarity, guided by a dissimilarity measure.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: When split quality is paramount and computational resources allow.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Can yield high-quality, low-dissimilarity splits.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Computationally intensive; sensitive to initial split.","category":"page"},{"location":"04-algorithms-overview/#Minimum/Maximum-Dissimilarity-Split","page":"04. Algorithms Overview","title":"Minimum/Maximum Dissimilarity Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Greedy cluster-based strategy that adds clusters to the training set based on minimal or maximal dissimilarity criteria until the desired fraction is reached.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: When a fast, cluster-aware splitting heuristic is sufficient.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Simpler and faster than full optimization.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Greedy nature may miss globally optimal arrangement; cluster sizes may vary.","category":"page"},{"location":"04-algorithms-overview/#Sphere-Exclusion-Split","page":"04. Algorithms Overview","title":"Sphere Exclusion Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Forms clusters by selecting a sample and excluding all neighbors within a specified radius. Entire clusters are then assigned to train or test to meet the split fraction.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Spatial or similarity-based data where locality grouping matters.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Intuitive radius control over cluster sizes.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Sensitive to radius choice; can produce imbalanced clusters.","category":"page"},{"location":"04-algorithms-overview/#Cluster-Shuffle-Split","page":"04. Algorithms Overview","title":"Cluster Shuffle Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Shuffles cluster labels and sequentially collects whole clusters into the training set until reaching the specified fraction, preserving inherent group structure.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Grouped or clustered data where splitting within groups is undesirable.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Maintains group integrity; introduces randomness.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"May overshoot or undershoot desired fraction if cluster sizes vary.","category":"page"},{"location":"04-algorithms-overview/#TargetPropertySplit","page":"04. Algorithms Overview","title":"TargetPropertySplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Partitions data into train/test sets by sorting samples according to a user-specified property (e.g., a column, a function of the sample, or a target value). The order argument (or alias) controls whether the largest/smallest property values are placed in the training set.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Useful for extrapolation or interpolation splits, e.g., when you want to train on the lowest (or highest) values of a property and test on the rest.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Simple and interpretable\nWorks with any property (target, feature, etc.)\nFlexible via property function and order aliases","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Not diversity-based; may not represent the full data distribution","category":"page"},{"location":"04-algorithms-overview/#TimeSplit","page":"04. Algorithms Overview","title":"TimeSplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Splits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Useful for time series or temporal data where you want to avoid splitting samples with the same timestamp.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Respects temporal order\nNever splits samples with the same date/time","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Fraction may not be exact due to grouping","category":"page"},{"location":"#DataSplits","page":"DataSplits","title":"DataSplits","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"DataSplits is a Julia library for rational train/test splitting algorithms. It provides a variety of strategies for splitting datasets. In several applications random selection is not an appropriate choice and may lead to overestimating model performance.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Strategy Purpose Complexity\nKennardStoneSplit Maximin split on X O(N²) time, O(N²) memory\nLazyKennardStoneSplit Same, streamed O(N²) time, O(N) mem\nSPXYSplit Joint X–y maximin (SPXY) O(N²) time, O(N²) mem\nOptiSimSplit Optimisable dissimilarity-based splitting O(N²) time, O(N²) memory\nMinimumDissimilaritySplit Greedy dissimilarity with one candidate O(N²) time, O(N²) memory\nMaximumDissimilaritySplit Greedy dissimilarity with full pool O(N²) time, O(N²) memory\nClusterShuffleSplit Cluster-based shuffle split O(N²) time, O(N²) memory\nClusterStratifiedSplit Cluster-based stratified split (equal, proportional, Neyman). Selects a quota per cluster, then splits into train/test according to user fraction. O(N²) time, O(N²) memory","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"All splitting strategies in DataSplits are designed to work with any AbstractArray, including those with non-standard axes. This is achieved by mapping user indices to internal positions, ensuring correctness and extensibility for all data types.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"julia> using DataSplits, Distances\n\njulia> train, test = split(X, KennardStoneSplit(0.8))\njulia> train, test = split((X, y), SPXYSplit(0.7; metric = Cityblock()))\njulia> train, test = split(X, ClusterStratifiedSplit(clusters, :equal; n=4, frac=0.7))","category":"page"}]
}
