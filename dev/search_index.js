var documenterSearchIndex = {"docs":
[{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#DataSplits.ClusterShuffleSplit","page":"Reference","title":"DataSplits.ClusterShuffleSplit","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data; rng)\n\nGroup-aware train/test splitter: accepts either:\n\nA precomputed ClusteringResult.\nA clustering function f(data) that returns one.\n\nAt construction, clustering is executed so the strategy always holds a ClusteringResult.\n\nArguments:\n\nres or `f(...)\nfrac: fraction of samples in the training set (0 < frac < 1).\n\nThis splitter shuffles cluster IDs and accumulates whole clusters until frac * N samples are in the train set, then returns (train_idx, test_idx).\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.KennardStoneSplit","page":"Reference","title":"DataSplits.KennardStoneSplit","text":"KennardStoneSplit{T} <: SplitStrategy\n\nA splitting strategy implementing the Kennard-Stone algorithm for train/test splitting.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric to use (default: Euclidean())\n\nExamples\n\n```julia\n\nCreate a splitter with 80% training data using Euclidean distance\n\nsplitter = KennardStoneSplit(0.8)\n\nCreate a splitter with custom metric\n\nusing Distances splitter = KennardStoneSplit(0.7, Cityblock())\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.OptiSimSplit","page":"Reference","title":"DataSplits.OptiSimSplit","text":"OptiSimSplit(frac; n_clusters = 10, max_subsample_size, distance_cutoff = 0.10,\n             metric = Euclidean(), random_state = 42)\n\nImplementation of OptiSim (Clark 1998, J. Chem. Inf. Comput. Sci.), an optimisable K‑dissimilarity selection.\n\nfrac – fraction of samples to return in the training subset\nn_clusters = M – requested cluster/selection‑set size\nmax_subsample_size = K – size of the temporary sub‑sample (default: max(1, ceil(Int, 0.05N)))\ndistance_cutoff = c – two points are “similar” if their distance < c\nmetric – any Distances.jl metric\nrandom_state – seed for the RNG\n\nThe splitter requires both an X matrix and target vector y when calling split.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SPXYSplit-Tuple{Real}","page":"Reference","title":"DataSplits.SPXYSplit","text":"SPXYSplit(frac; metric = Euclidean())\n\nCreate an SPXY splitter – the variant of Kennard–Stone in which the distance matrix is the element‑wise sum of\n\nthe (normalised) pairwise distance matrix of the feature matrix X\nplus the (normalised) pairwise distance matrix of the response vector y.\n\nfrac is the fraction of samples that will end up in the training subset.\n\nnote: Note\nsplit must be called with a 2‑tuple (X, y) or with positional arguments split(X, y, strategy); calling split(X, strategy) will raise a MethodError, because y is mandatory for SPXY.\n\nArguments\n\nname type meaning\nfrac Real (0 < frac < 1) training‑set fraction\nmetric [Distances.SemiMetric] distance metric used for both X and y\n\nSee also\n\nKennardStoneSplit — the classical variant that uses only X.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.SphereExclusionResult","page":"Reference","title":"DataSplits.SphereExclusionResult","text":"SphereExclusionResult\n\nResult of sphere exclusion clustering.\n\nFields:\n\nassignments::Vector{Int}: cluster index per point.\nradius::Float64: exclusion radius.\nmetric::Distances.SemiMetric: distance metric.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits._split-Tuple{AbstractArray, AbstractVector, SPXYSplit}","page":"Reference","title":"DataSplits._split","text":"_split(X, y, strategy::SPXYSplit; rng = Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nSplit a feature matrix X and response vector y into train/test subsets using the SPXY algorithm:\n\nBuild the joint distance matrix D = D_X + D_Y  (see SPXYSplit for details).\nRun the Kennard–Stone maximin procedure on D.\nReturn two sorted index vectors (train_idx, test_idx).\n\nArguments\n\nname type requirement\nX AbstractMatrix size(X, 1) == length(y)\ny AbstractVector \nstrategy SPXYSplit created with SPXYSplit(frac; metric)\nrng random‑number source unused here but kept for API symmetry\n\nReturns\n\nTwo Vector{Int} with the row indices of X (and the corresponding entries of y) that belong to the training and test subsets.\n\nThe indices are axis‑correct — if X is an OffsetMatrix whose first row is index 0, the returned indices will also start at 0.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, KennardStoneSplit}","page":"Reference","title":"DataSplits._split","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nOptimized in-memory Kennard-Stone algorithm using precomputed distance matrix. Best for small-to-medium datasets where O(N²) memory is acceptable.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, LazyKennardStoneSplit}","page":"Reference","title":"DataSplits._split","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nKennard-Stone (CADEX) algorithm for optimal train/test splitting using maximin strategy. Memory-optimized implementation with O(N) storage. Useful when working with large datasets where the NxN distance matrix does not fit memory. When working with small datasets, use the traditional implementation.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.distance_matrix-Tuple{Any, Distances.PreMetric}","page":"Reference","title":"DataSplits.distance_matrix","text":"distance_matrix(X, metric::PreMetric)\n\nCompute the full symmetric pairwise distance matrix D for the dataset X using the given metric. This function uses get_sample to access samples, ensuring compatibility with any custom array type that implements get_sample and sample_indices.\n\nReturns a matrix D such that D[i, j] = metric(xᵢ, xⱼ) and D[i, j] == D[j, i].\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_maximin_element-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Union{AbstractSet{Int64}, AbstractVector{Int64}}, Union{AbstractSet{Int64}, AbstractVector{Int64}}}} where T<:Real","page":"Reference","title":"DataSplits.find_maximin_element","text":"find_maximin_element(distances::AbstractMatrix{T},\n                    source_set::AbstractVector{Int},\n                    reference_set::AbstractVector{Int}) -> Int\n\nFind the element in source_set that maximizes the minimum distance to all elements in reference_set.\n\nArguments\n\ndistances::AbstractMatrix{T}: Precomputed, symmetric pairwise distance matrix.\nsource_set::AbstractVector{Int}: Set of items to evaluate.\nreference_set::AbstractVector{Int}: Set of items to compare against.\n\nReturns\n\nInt: The index in source_set that is farthest from its nearest neighbour in reference_set.\n\nNotes\n\nIf reference_set is empty, throws ArgumentError\nBreaks ties by returning the first maximum\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_most_distant_pair-Tuple{AbstractMatrix}","page":"Reference","title":"DataSplits.find_most_distant_pair","text":"find_most_distant_pair(D::AbstractMatrix) → (i, j)\n\nFinds indices of most distant pair in precomputed distance matrix.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.get_sample-Tuple{AbstractArray, Any}","page":"Reference","title":"DataSplits.get_sample","text":"get_sample(A::AbstractArray, idx)\n\nPublic API for getting samples that handles any valid index type. Dispatches to internalget_sample after index conversion.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.is_sample_array-Tuple{Type}","page":"Reference","title":"DataSplits.is_sample_array","text":"is_sample_array(::Type{T}) -> Bool\n\nTrait method that tells whether type T behaves like a sample‑indexed array that supports sample_indices, _get_sample, etc.\n\nYou may extend this for custom containers to ensure compatibility with OptiSim or other sampling methods.\n\nDefaults to false unless specialized.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sample_indices-Tuple{AbstractArray}","page":"Reference","title":"DataSplits.sample_indices","text":"sample_indices(A::AbstractArray) -> AbstractVector\n\nReturn the list of sample-level indices used to address elements in A.\n\nThis method defines the \"sample axis\" (typically axis 1) and determines how your splitting/sampling algorithms enumerate data points.\n\nDefault\n\nFor standard arrays, returns axes(A, 1).\n\nExtension\n\nTo support non-standard arrays (e.g., views, custom wrappers), you may extend this method to expose logical sample indices:\n\nBase.sample_indices(a::MyFancyArray) = 1:length(a.ids)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sphere_exclusion-Tuple{Any}","page":"Reference","title":"DataSplits.sphere_exclusion","text":"sphere_exclusion(data; radius::Real, metric::Distances.SemiMetric=Euclidean()) -> SphereExclusionResult\n\nCluster samples in data by sphere exclusion:\n\nCompute full pairwise distance matrix and normalize values to [0,1].\nWhile unassigned samples remain:\nPick first unassigned sample i.\nAll unassigned samples j with normalized distance D[i,j] <= radius form a cluster.\nMark them assigned and increment cluster ID.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split-Tuple{Any, SplitStrategy}","page":"Reference","title":"DataSplits.split","text":"split(data, strategy; rng=Random.default_rng()) → (train, test)\n\nSplit data into train/test sets according to strategy.\n\n\n\n\n\n","category":"method"},{"location":"#DataSplits","page":"DataSplits","title":"DataSplits","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Documentation for DataSplits.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"A tiny Julia library for rational train/test splitting algorithms:","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Strategy Purpose Complexity\nKennardStoneSplit Maximin split on X O(N²) time, O(N²) memory\nLazyKennardStoneSplit Same, streamed O(N²) time, O(N) mem\nSPXYSplit Joint X–y maximin (SPXY) O(N²) time, O(N²) mem\nOptiSimSplit Optimisable dissimilarity-based splitting O(N²) time, O(N²) memory\nMinimumDissimilaritySplit Greedy dissimilarity with one candidate O(N²) time, O(N²) memory\nMaximumDissimilaritySplit Greedy dissimilarity with full pool O(N²) time, O(N²) memory","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"julia> using DataSplits, Distances\n\njulia> train, test = split(X, KennardStoneSplit(0.8))\njulia> train, test = split((X, y), SPXYSplit(0.7; metric = Cityblock()))","category":"page"}]
}
