var documenterSearchIndex = {"docs":
[{"location":"07-kennard-stone/#07.-Kennard–Stone-Split","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"","category":"section"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Constructor:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"KennardStoneSplit(frac::Real; metric::SemiMetric=Euclidean())","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Description: The Kennard–Stone algorithm is a deterministic method for selecting a representative training set from a dataset. It works by iteratively choosing the sample that is farthest (in feature space) from all previously selected samples, starting from the most distant pair. This ensures that the training set covers the full range of the feature space, making it especially useful for small or structured datasets where random splits may not be representative.","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"When to use:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"When you want a training set that is diverse and covers the feature space uniformly.\nFor small to medium datasets where overfitting is a concern.","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"When not to use:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"For very large datasets (memory intensive).\nWhen you need to account for the target variable (see SPXY).","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Arguments:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"frac: fraction of data in training set (0 < frac < 1).\nmetric: distance metric (default Euclidean).","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Usage:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"using DataSplits, Distances\nsplitter = KennardStoneSplit(0.8)\ntrain, test = split(X, splitter)","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Pros:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Deterministic and reproducible.\nEnsures diverse, well-spread training set.","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Cons:","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"Memory-intensive for large datasets (stores full distance matrix).\nIgnores the target variable.","category":"page"},{"location":"07-kennard-stone/","page":"07. Kennard–Stone Split","title":"07. Kennard–Stone Split","text":"See Also: CADEXSplit (alias)","category":"page"},{"location":"09-spxy/#09.-SPXY-Split","page":"09. SPXY Split","title":"09. SPXY Split","text":"","category":"section"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Constructor:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"SPXYSplit(frac::Real; metric::SemiMetric=Euclidean())","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Description: The SPXY algorithm extends Kennard–Stone by considering both the feature matrix (X) and the target vector (y) when splitting data. It constructs a joint distance matrix as the sum of normalized pairwise distances in X and y, then applies the maximin selection. This ensures the training set is diverse in both predictors and response, which is especially important for regression tasks where the target distribution matters.","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"When to use:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"For regression problems where you want the training set to represent both features and target.\nWhen target stratification is important.","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"When not to use:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"For classification tasks (unless you encode the target appropriately).\nFor very large datasets (O(N²) memory).","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Arguments:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"frac: training fraction.\nmetric: distance metric for both X and y.","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Usage:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"using DataSplits, Distances\ntrain, test = split((X, y), SPXYSplit(0.7; metric=Cityblock()))","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Pros:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Balances representation of predictors and response.\nUseful for regression and continuous targets.","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Cons:","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"May overweight target variation if y is noisy.\nStill requires O(N²) memory and computation.","category":"page"},{"location":"09-spxy/","page":"09. SPXY Split","title":"09. SPXY Split","text":"Note: split(X, strategy) without y will error.","category":"page"},{"location":"06-examples-tutorials/#06.-Examples-and-Tutorials","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"","category":"section"},{"location":"06-examples-tutorials/#Example:-SPXY-Split-on-Custom-Data","page":"06. Examples & Tutorials","title":"Example: SPXY Split on Custom Data","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"using DataSplits, Distances\n# Suppose `df` is a DataFrame with features and target\ndf = DataFrame(rand(100, 5), :auto)\ny = rand(100)\ntrain, test = split((Matrix(df), y), SPXYSplit(0.75))","category":"page"},{"location":"06-examples-tutorials/#Example:-Custom-Splitter-Implementation","page":"06. Examples & Tutorials","title":"Example: Custom Splitter Implementation","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"Suppose you want to create a splitter that always assigns the first 80% of samples to train:","category":"page"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"struct First80Split <: SplitStrategy end\n\nfunction first80(N, s, rng, data)\n    cut = floor(Int, 0.8 * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, ::First80Split; rng=nothing)\n    split_with_positions(data, nothing, first80; rng=rng)\nend","category":"page"},{"location":"06-examples-tutorials/#Example:-Group-aware-Splitting","page":"06. Examples & Tutorials","title":"Example: Group-aware Splitting","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"06. Examples & Tutorials","title":"06. Examples & Tutorials","text":"using DataSplits\n# Suppose you have cluster assignments for your data\nclusters = [rand(1:5) for _ in 1:100]\nusing Clustering\nres = Clustering.ClusteringResult(clusters)\ntrain, test = split(X, ClusterShuffleSplit(res, 0.7))","category":"page"},{"location":"10-optisim/#10.-OptiSim-Split","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"","category":"section"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Constructor:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"OptiSimSplit(frac::Real; max_subsample_size=0, distance_cutoff=0.35, metric=Euclidean())","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Description: OptiSim is an iterative, optimisable splitting algorithm that aims to maximize the diversity of the training set by minimizing within-set similarity. At each step, it generates a temporary subsample of candidates and selects the one most dissimilar to the current training set, repeating until the desired fraction is reached. The algorithm is flexible and can be tuned for speed or quality by adjusting the subsample size and distance cutoff.","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"When to use:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"When you want a highly diverse training set and can afford extra computation.\nFor chemical, biological, or other scientific datasets where diversity is critical.","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"When not to use:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"For very large datasets (computationally intensive).\nWhen you need a fast, simple split.","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Arguments:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"frac: fraction of training samples.\nmax_subsample_size: size of candidate pool at each step.\ndistance_cutoff: threshold for similarity filtering.\nmetric: distance metric.","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Usage:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"using DataSplits\ntrain, test = split(X, OptiSimSplit(0.8; max_subsample_size=50))","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Pros:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Produces high-quality, diverse splits.\nFlexible and tunable.","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Cons:","category":"page"},{"location":"10-optisim/","page":"10. OptiSim Split","title":"10. OptiSim Split","text":"Computationally intensive; sensitive to parameters.","category":"page"},{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#DataSplits.ClusterShuffleSplit","page":"Reference","title":"DataSplits.ClusterShuffleSplit","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data; rng)\n\nGroup-aware train/test splitter: accepts either:\n\nA precomputed ClusteringResult.\nA clustering function f(data) that returns one.\n\nAt construction, clustering is executed so the strategy always holds a ClusteringResult.\n\nArguments:\n\nres or `f(...)\nfrac: fraction of samples in the training set (0 < frac < 1).\n\nThis splitter shuffles cluster IDs and accumulates whole clusters until frac * N samples are in the train set, then returns (train_idx, test_idx).\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.KennardStoneSplit","page":"Reference","title":"DataSplits.KennardStoneSplit","text":"KennardStoneSplit{T} <: SplitStrategy\n\nA splitting strategy implementing the Kennard-Stone algorithm for train/test splitting.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric to use (default: Euclidean())\n\nExamples\n\n```julia\n\nCreate a splitter with 80% training data using Euclidean distance\n\nsplitter = KennardStoneSplit(0.8)\n\nCreate a splitter with custom metric\n\nusing Distances splitter = KennardStoneSplit(0.7, Cityblock())\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.OptiSimSplit","page":"Reference","title":"DataSplits.OptiSimSplit","text":"OptiSimSplit(frac; n_clusters = 10, max_subsample_size, distance_cutoff = 0.10,\n             metric = Euclidean(), random_state = 42)\n\nImplementation of OptiSim (Clark 1998, J. Chem. Inf. Comput. Sci.), an optimisable K‑dissimilarity selection.\n\nfrac – fraction of samples to return in the training subset\nn_clusters = M – requested cluster/selection‑set size\nmax_subsample_size = K – size of the temporary sub‑sample (default: max(1, ceil(Int, 0.05N)))\ndistance_cutoff = c – two points are “similar” if their distance < c\nmetric – any Distances.jl metric\nrandom_state – seed for the RNG\n\nThe splitter requires both an X matrix and target vector y when calling split.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SPXYSplit-Tuple{Real}","page":"Reference","title":"DataSplits.SPXYSplit","text":"SPXYSplit(frac; metric = Euclidean())\n\nCreate an SPXY splitter – the variant of Kennard–Stone in which the distance matrix is the element‑wise sum of\n\nthe (normalised) pairwise distance matrix of the feature matrix X\nplus the (normalised) pairwise distance matrix of the response vector y.\n\nfrac is the fraction of samples that will end up in the training subset.\n\nnote: Note\nsplit must be called with a 2‑tuple (X, y) or with positional arguments split(X, y, strategy); calling split(X, strategy) will raise a MethodError, because y is mandatory for SPXY.\n\nArguments\n\nname type meaning\nfrac Real (0 < frac < 1) training‑set fraction\nmetric [Distances.SemiMetric] distance metric used for both X and y\n\nSee also\n\nKennardStoneSplit — the classical variant that uses only X.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.SphereExclusionResult","page":"Reference","title":"DataSplits.SphereExclusionResult","text":"SphereExclusionResult\n\nResult of sphere exclusion clustering.\n\nFields:\n\nassignments::Vector{Int}: cluster index per point.\nradius::Float64: exclusion radius.\nmetric::Distances.SemiMetric: distance metric.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TargetPropertySplit","page":"Reference","title":"DataSplits.TargetPropertySplit","text":"TargetPropertySplit{T} <: SplitStrategy\n\nA splitting strategy that partitions a 1D property array into train/test sets by sorting the property values.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc, :desc, :high, :low, :largest, :smallest, etc.\n\nExamples\n\nsplit(y, TargetPropertyHigh(0.8))\nsplit(X[:, 3], TargetPropertyLow(0.5))\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TimeSplit","page":"Reference","title":"DataSplits.TimeSplit","text":"TimeSplit{T} <: SplitStrategy\n\nSplits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc (oldest in train, default), :desc (newest in train)\n\nExamples\n\nsplit(dates, TimeSplitOldest(0.7))\nsplit(dates, TimeSplitNewest(0.3))\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits._split-Tuple{AbstractArray, AbstractVector, SPXYSplit}","page":"Reference","title":"DataSplits._split","text":"_split(X, y, strategy::SPXYSplit; rng = Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nSplit a feature matrix X and response vector y into train/test subsets using the SPXY algorithm:\n\nBuild the joint distance matrix D = D_X + D_Y  (see SPXYSplit for details).\nRun the Kennard–Stone maximin procedure on D.\nReturn two sorted index vectors (train_idx, test_idx).\n\nArguments\n\nname type requirement\nX AbstractMatrix size(X, 1) == length(y)\ny AbstractVector \nstrategy SPXYSplit created with SPXYSplit(frac; metric)\nrng random‑number source unused here but kept for API symmetry\n\nReturns\n\nTwo Vector{Int} with the row indices of X (and the corresponding entries of y) that belong to the training and test subsets.\n\nThe indices are axis‑correct — if X is an OffsetMatrix whose first row is index 0, the returned indices will also start at 0.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.distance_matrix-Tuple{Any, Distances.PreMetric}","page":"Reference","title":"DataSplits.distance_matrix","text":"distance_matrix(X, metric::PreMetric)\n\nCompute the full symmetric pairwise distance matrix D for the dataset X using the given metric. This function uses get_sample to access samples, ensuring compatibility with any custom array type that implements get_sample and sample_indices.\n\nReturns a matrix D such that D[i, j] = metric(xᵢ, xⱼ) and D[i, j] == D[j, i].\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_maximin_element-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Union{AbstractSet{Int64}, AbstractVector{Int64}}, Union{AbstractSet{Int64}, AbstractVector{Int64}}}} where T<:Real","page":"Reference","title":"DataSplits.find_maximin_element","text":"find_maximin_element(distances::AbstractMatrix{T},\n                    source_set::AbstractVector{Int},\n                    reference_set::AbstractVector{Int}) -> Int\n\nFind the element in source_set that maximizes the minimum distance to all elements in reference_set.\n\nArguments\n\ndistances::AbstractMatrix{T}: Precomputed, symmetric pairwise distance matrix.\nsource_set::AbstractVector{Int}: Set of items to evaluate.\nreference_set::AbstractVector{Int}: Set of items to compare against.\n\nReturns\n\nInt: The index in source_set that is farthest from its nearest neighbour in reference_set.\n\nNotes\n\nIf reference_set is empty, throws ArgumentError\nBreaks ties by returning the first maximum\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_most_distant_pair-Tuple{AbstractMatrix}","page":"Reference","title":"DataSplits.find_most_distant_pair","text":"find_most_distant_pair(D::AbstractMatrix) → (i, j)\n\nFinds indices of most distant pair in precomputed distance matrix.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.get_sample-Tuple{AbstractArray, Any}","page":"Reference","title":"DataSplits.get_sample","text":"get_sample(A::AbstractArray, idx)\n\nPublic API for getting samples that handles any valid index type. Dispatches to internalget_sample after index conversion.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.is_sample_array-Tuple{Type}","page":"Reference","title":"DataSplits.is_sample_array","text":"is_sample_array(::Type{T}) -> Bool\n\nTrait method that tells whether type T behaves like a sample‑indexed array that supports sample_indices, _get_sample, etc.\n\nYou may extend this for custom containers to ensure compatibility with OptiSim or other sampling methods.\n\nDefaults to false unless specialized.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.kennardstone-NTuple{4, Any}","page":"Reference","title":"DataSplits.kennardstone","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nOptimized in-memory Kennard-Stone algorithm using precomputed distance matrix. Best for small-to-medium datasets where O(N²) memory is acceptable.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.lazy_kennard_stone-NTuple{4, Any}","page":"Reference","title":"DataSplits.lazy_kennard_stone","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nKennard-Stone (CADEX) algorithm for optimal train/test splitting using maximin strategy. Memory-optimized implementation with O(N) storage. Useful when working with large datasets where the NxN distance matrix does not fit memory. When working with small datasets, use the traditional implementation.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sample_indices-Tuple{AbstractArray}","page":"Reference","title":"DataSplits.sample_indices","text":"sample_indices(A::AbstractArray) -> AbstractVector\n\nReturn a vector of valid sample indices for A`, supporting all AbstractArray types.\n\nThis method defines the \"sample axis\" (typically axis 1) and determines how your splitting/sampling algorithms enumerate data points.\n\nDefault\n\nFor standard arrays, returns axes(A, 1).\n\nExtension\n\nTo support non-standard arrays (e.g., views, custom wrappers), you may extend this method to expose logical sample indices:\n\nBase.sample_indices(a::MyFancyArray) = 1:length(a.ids)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sphere_exclusion-Tuple{Any}","page":"Reference","title":"DataSplits.sphere_exclusion","text":"sphere_exclusion(data; radius::Real, metric::Distances.SemiMetric=Euclidean()) -> SphereExclusionResult\n\nCluster samples in data by sphere exclusion:\n\nCompute full pairwise distance matrix and normalize values to [0,1].\nWhile unassigned samples remain:\nPick first unassigned sample i.\nAll unassigned samples j with normalized distance D[i,j] <= radius form a cluster.\nMark them assigned and increment cluster ID.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split-Tuple{Any, SplitStrategy}","page":"Reference","title":"DataSplits.split","text":"split(data, strategy; rng=Random.default_rng()) → (train, test)\n\nSplit data into train/test sets according to strategy.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split_with_positions-Tuple{Any, Any, Any}","page":"Reference","title":"DataSplits.split_with_positions","text":"split_with_positions(data, s, core_algorithm; rng=Random.default_rng(), args...)\n\nGeneric wrapper for split strategies. Handles mapping between user indices and 1:N positions.\n\ndata: The user’s data array.\ns: The split strategy object.\ncore_algorithm: Function (N, s, rng, data, args...) -> (trainpos, testpos)\n\nReturns: (trainidx, testidx) as indices valid for data.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.targetpropertysplit-Tuple{Any, Any, Any, AbstractVector}","page":"Reference","title":"DataSplits.targetpropertysplit","text":"targetpropertysplit(N, s, rng, data)\n\nSorts the property array and splits into train/test according to s.frac and s.order.\n\n\n\n\n\n","category":"method"},{"location":"03-core-api-reference/#03.-Core-API-Reference","page":"03. Core API Reference","title":"03. Core API Reference","text":"","category":"section"},{"location":"03-core-api-reference/#split","page":"03. Core API Reference","title":"split","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"split(data, strategy; rng)","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"Dispatches to strategy._split. Returns (train, test) index vectors.","category":"page"},{"location":"03-core-api-reference/#split*with*positions","page":"03. Core API Reference","title":"splitwithpositions","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"split_with_positions(data, s, core_algorithm; rng=Random.default_rng())","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"A utility function that handles mapping between user-facing indices (which may be non-1-based or non-contiguous) and internal 1:N positions. The core_algorithm should operate on positions 1:N and return (train_pos, test_pos), which are then mapped back to the correct indices for the user.","category":"page"},{"location":"03-core-api-reference/#SplitStrategy-Interface","page":"03. Core API Reference","title":"SplitStrategy Interface","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"To add a new strategy, subtype SplitStrategy and implement:","category":"page"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"_split(data, s::YourStrategy; rng)","category":"page"},{"location":"03-core-api-reference/#Utility-Functions","page":"03. Core API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"sample_indices(data): returns iterable indices of samples (default: 1:size(data,1), but now robust to any AbstractArray axes).\nValidFraction: bounds-checked fraction type for split ratios.","category":"page"},{"location":"03-core-api-reference/#Example:-Custom-Splitter-(new-pattern)","page":"03. Core API Reference","title":"Example: Custom Splitter (new pattern)","text":"","category":"section"},{"location":"03-core-api-reference/","page":"03. Core API Reference","title":"03. Core API Reference","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    split_with_positions(data, s, mysplit; rng=rng)\nend","category":"page"},{"location":"02-getting-started/#02.-Getting-Started","page":"02. Getting Started","title":"02. Getting Started","text":"","category":"section"},{"location":"02-getting-started/#Basic-API","page":"02. Getting Started","title":"Basic API","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"split(data, strategy): main entry point.\ndata: array, tuple (X,) or (X, y).\nstrategy: subtype of SplitStrategy.","category":"page"},{"location":"02-getting-started/#Data-Formats","page":"02. Getting Started","title":"Data Formats","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"Accepts matrices, tables, or custom types implementing sample_indices.","category":"page"},{"location":"02-getting-started/#Robust-Index-Handling","page":"02. Getting Started","title":"Robust Index Handling","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"All splitting strategies in DataSplits are robust to arrays with arbitrary axes (e.g., OffsetArrays, SubArrays, etc.). The library automatically handles mapping between user-facing indices and internal positions, so you can use any AbstractArray as input.","category":"page"},{"location":"02-getting-started/#Randomness-Control","page":"02. Getting Started","title":"Randomness Control","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"Pass rng keyword to strategies supporting it, e.g. split(X, RandomSplit(0.7); rng=123).","category":"page"},{"location":"02-getting-started/#Example:-Custom-Data-Type","page":"02. Getting Started","title":"Example: Custom Data Type","text":"","category":"section"},{"location":"02-getting-started/","page":"02. Getting Started","title":"02. Getting Started","text":"To use your own data type, implement sample_indices(data) and get_sample(data, i).","category":"page"},{"location":"13-sphere-exclusion/#13.-Sphere-Exclusion-Split","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Constructor:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"SphereExclusionSplit(frac::Real; radius::Real, metric=Euclidean())","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Description: Sphere Exclusion Split is a clustering-based splitting strategy. It works by iteratively picking an unassigned sample and forming a cluster of all points within a specified radius (in normalized distance). This process repeats until all samples are assigned to clusters. Clusters are then assigned to the training set until the desired fraction is reached. This method is especially useful for spatial or similarity-based data, where you want to avoid splitting local neighborhoods.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When to use:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"For spatial, chemical, or biological data where locality matters.\nWhen you want to avoid splitting similar samples across train/test.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When not to use:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"When you do not know how to set a meaningful radius.\nFor data with no meaningful distance metric.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Arguments:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"frac: training fraction.\nradius: normalized distance threshold [0,1].\nmetric: distance metric.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Usage:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"using DataSplits\ntrain, test = split(X, SphereExclusionSplit(0.7; radius=0.2))","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Pros:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Intuitive spatial grouping.\nAvoids splitting local neighborhoods.","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Cons:","category":"page"},{"location":"13-sphere-exclusion/","page":"13. Sphere Exclusion Split","title":"13. Sphere Exclusion Split","text":"Sensitive to radius; cluster sizes may be uneven.\nMay not work well for high-dimensional or non-metric data.","category":"page"},{"location":"08-lazy-kennard-stone/#08.-Lazy-Kennard–Stone-Split","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"","category":"section"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Constructor:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"LazyKennardStoneSplit(frac::Real; metric::SemiMetric=Euclidean())","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Description: The Lazy Kennard–Stone algorithm is a memory-efficient variant of the classic Kennard–Stone split. Instead of precomputing and storing the full distance matrix, it computes distances on the fly as needed. This makes it suitable for larger datasets where the standard approach would be infeasible due to memory constraints. The selection logic is otherwise identical: iteratively select the sample farthest from those already chosen.","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"When to use:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"When your dataset is too large to fit a full distance matrix in memory.\nWhen you want deterministic, diverse splits but have limited resources.","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"When not to use:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"For very small datasets (classic Kennard–Stone is faster).\nWhen you need to account for the target variable (see SPXY).","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Arguments:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"frac: fraction for training set.\nmetric: distance metric (default Euclidean).","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Usage:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"using DataSplits\nsplitter = LazyKennardStoneSplit(0.8)\ntrain, test = split(X, splitter)","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Pros:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Lower memory footprint than classic Kennard–Stone.\nStill deterministic and diverse.","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"Cons:","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"More CPU time due to repeated distance calculations.","category":"page"},{"location":"08-lazy-kennard-stone/","page":"08. Lazy Kennard–Stone Split","title":"08. Lazy Kennard–Stone Split","text":"See Also: KennardStoneSplit","category":"page"},{"location":"05-extending-data-splits/#05.-Extending-DataSplits","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"","category":"section"},{"location":"05-extending-data-splits/#Custom-Clustering","page":"05. Extending DataSplits","title":"Custom Clustering","text":"","category":"section"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"To add a new clustering algorithm:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Subtype ClusteringResult.\nImplement assignments, nclusters, counts, wcounts for your result type.\nRegister a clustering function returning your result.","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Example:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"struct MyClusteringResult <: ClusteringResult\n  assignments::Vector{Int}\nend\nassignments(r::MyClusteringResult) = r.assignments\nnclusters(r::MyClusteringResult) = maximum(r.assignments)\n# ...","category":"page"},{"location":"05-extending-data-splits/#Custom-Splitter","page":"05. Extending DataSplits","title":"Custom Splitter","text":"","category":"section"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"To add a new splitting strategy:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Subtype SplitStrategy.\nImplement a core function (e.g., mysplit(N, s, rng, data)) that returns (train_pos, test_pos) for positions 1:N.\nImplement _split(data, s; rng) to call split_with_positions(data, s, mysplit; rng=rng).\nUse ValidFraction for fraction validation.","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"Example:","category":"page"},{"location":"05-extending-data-splits/","page":"05. Extending DataSplits","title":"05. Extending DataSplits","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    split_with_positions(data, s, mysplit; rng=rng)\nend","category":"page"},{"location":"11-minimum-dissimilarity/#11.-Minimum-Dissimilarity-Split","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"","category":"section"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Constructor:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"MinimumDissimilaritySplit(frac::Real; distance_cutoff=0.35, metric=Euclidean())","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Description: The Minimum Dissimilarity Split is a greedy, fast variant of OptiSim. At each step, it considers only one candidate sample and adds the one that is most dissimilar to the current training set, repeating until the desired fraction is reached. This approach is much faster than full OptiSim but may not achieve the same level of diversity. It is implemented as an alias for OptiSim with max_subsample_size=1.","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"When to use:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"When you need a quick, diversity-aware split for large datasets.\nWhen computational resources are limited.","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"When not to use:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"When maximum diversity is critical (see Maximum Dissimilarity or OptiSim).","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Arguments:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"frac: fraction for training set.\ndistance_cutoff: similarity threshold.\nmetric: distance metric.","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Usage:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"using DataSplits\ntrain, test = split(X, MinimumDissimilaritySplit(0.8))","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Pros:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Fast and simple.\nSome diversity in training set.","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Cons:","category":"page"},{"location":"11-minimum-dissimilarity/","page":"11. Minimum Dissimilarity Split","title":"11. Minimum Dissimilarity Split","text":"Greedy; may miss optimal diversity.","category":"page"},{"location":"01-introduction/#01.-Introduction","page":"01. Introduction","title":"01. Introduction","text":"","category":"section"},{"location":"01-introduction/#Motivation-and-Features","page":"01. Introduction","title":"Motivation and Features","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"DataSplits provides rational train/test splitting algorithms and clustering-based pre-processing for reproducible model evaluation. Unlike random splits, these methods ensure that the training and test sets are representative and diverse, which is especially important for small or structured datasets.","category":"page"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"Key Features:","category":"page"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"Multiple splitting strategies: maximin, clustering-based, group-aware, and more\nExtensible API for custom strategies\nWorks with arrays, tuples, and custom data types","category":"page"},{"location":"01-introduction/#Installation","page":"01. Introduction","title":"Installation","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"] add https://github.com/davide-grheco/DataSplits.jl","category":"page"},{"location":"01-introduction/#When-to-Use-DataSplits","page":"01. Introduction","title":"When to Use DataSplits","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"When random splits are not enough (e.g., small datasets, strong structure)\nWhen you need reproducible, rational splits for benchmarking\nWhen you want to preserve group or cluster structure in your splits","category":"page"},{"location":"01-introduction/#Quickstart-Example","page":"01. Introduction","title":"Quickstart Example","text":"","category":"section"},{"location":"01-introduction/","page":"01. Introduction","title":"01. Introduction","text":"using DataSplits, Distances\n\n# Simple Kennard–Stone split\ntrain, test = split(X, KennardStoneSplit(0.8))\n\n# SPXY split on features and target\ntrain, test = split((X, y), SPXYSplit(0.7; metric=Cityblock()))","category":"page"},{"location":"12-maximum-dissimilarity/#12.-Maximum-Dissimilarity-Split","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"","category":"section"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Constructor:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"MaximumDissimilaritySplit(frac::Real; distance_cutoff=0.35, metric=Euclidean())","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Description: The Maximum Dissimilarity Split is a greedy, diversity-maximizing variant of OptiSim. At each step, it considers all remaining candidate samples and selects the one that is maximally dissimilar (i.e., has the largest minimum distance) to the current training set. This approach is slower than Minimum Dissimilarity but can yield a more diverse training set. It is implemented as an alias for OptiSim with max_subsample_size=N.","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"When to use:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"When you want the most diverse training set possible and can afford extra computation.\nFor scientific or chemical datasets where outlier coverage is important.","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"When not to use:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"When speed is more important than diversity.\nIf you want to avoid including outliers (pre-filter them first).","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Arguments:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"frac: training fraction.\ndistance_cutoff: similarity threshold.\nmetric: distance metric.","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Usage:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"using DataSplits\ntrain, test = split(X, MaximumDissimilaritySplit(0.8))","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Pros:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Promotes maximum diversity in training set.\nUseful for challenging, representative splits.","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Cons:","category":"page"},{"location":"12-maximum-dissimilarity/","page":"12. Maximum Dissimilarity Split","title":"12. Maximum Dissimilarity Split","text":"Higher computational cost.\nMay include outliers if present in data.","category":"page"},{"location":"14-cluster-shuffle/#14.-Cluster-Shuffle-Split","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Constructor:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data)","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Description: Cluster Shuffle Split is a group-aware splitting strategy. It either takes a precomputed clustering result or a function to generate one, then shuffles the cluster labels and accumulates whole clusters into the training set until the desired fraction is reached. This approach is ideal for grouped or clustered data where splitting within groups would break structure or introduce leakage.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When to use:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When your data has natural groups or clusters (e.g., patients, molecules, batches).\nWhen you want to avoid splitting groups across train/test.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When not to use:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"When clusters are very imbalanced in size (fraction control is coarse).\nFor unstructured data with no meaningful groups.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Arguments:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"res or f,data: clustering result or function.\nfrac: training fraction.\nrng: optional RNG.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Usage:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"using DataSplits, Clustering\nres = sphere_exclusion(X; radius=0.3)\ntrain, test = split(X, ClusterShuffleSplit(res, 0.8))","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Pros:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Preserves group integrity.\nEasy to randomize splits.","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Cons:","category":"page"},{"location":"14-cluster-shuffle/","page":"14. Cluster Shuffle Split","title":"14. Cluster Shuffle Split","text":"Fraction control is coarse; may overshoot/undershoot target.\nCluster sizes may vary widely.","category":"page"},{"location":"04-algorithms-overview/#04.-Algorithms-Overview","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Each splitter returns two sets of indices (train, test), partitioning samples according to the chosen strategy. Choose based on dataset size, desired diversity, or grouping needs.","category":"page"},{"location":"04-algorithms-overview/#Kennard–Stone-Split","page":"04. Algorithms Overview","title":"Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Iteratively selects samples by choosing the point farthest from all previously chosen points in the feature space. This approach ensures the training set uniformly covers the distribution of predictors.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Ideal for small to medium datasets where uniform feature coverage improves model generalization.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Deterministic selection ensures reproducible splits.\nProvides diverse representation of feature space.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Requires storing and computing full distance matrix, which can be memory-intensive.","category":"page"},{"location":"04-algorithms-overview/#Lazy-Kennard–Stone-Split","page":"04. Algorithms Overview","title":"Lazy Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: A streaming variant of Kennard–Stone that computes distances on the fly, avoiding the full matrix. Suitable when memory is constrained.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Large datasets where memory is a bottleneck.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Reduced memory overhead.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"May incur extra computational overhead due to repeated distance computations.","category":"page"},{"location":"04-algorithms-overview/#SPXY-Split","page":"04. Algorithms Overview","title":"SPXY Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Extends Kennard–Stone to both features (X) and target variable (y), selecting samples that maximize combined spread. Ensures training set represents both predictors and response.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Regression tasks where preserving target distribution is critical.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Balances feature and response diversity.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Still requires pairwise distance computations; may overweight target variation.","category":"page"},{"location":"04-algorithms-overview/#OptiSim-Split","page":"04. Algorithms Overview","title":"OptiSim Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Applies iterative swapping of samples between train and test sets to minimize within-set dissimilarity, guided by a dissimilarity measure.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: When split quality is paramount and computational resources allow.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Can yield high-quality, low-dissimilarity splits.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Computationally intensive; sensitive to initial split.","category":"page"},{"location":"04-algorithms-overview/#Minimum/Maximum-Dissimilarity-Split","page":"04. Algorithms Overview","title":"Minimum/Maximum Dissimilarity Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Greedy cluster-based strategy that adds clusters to the training set based on minimal or maximal dissimilarity criteria until the desired fraction is reached.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: When a fast, cluster-aware splitting heuristic is sufficient.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Simpler and faster than full optimization.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Greedy nature may miss globally optimal arrangement; cluster sizes may vary.","category":"page"},{"location":"04-algorithms-overview/#Sphere-Exclusion-Split","page":"04. Algorithms Overview","title":"Sphere Exclusion Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Forms clusters by selecting a sample and excluding all neighbors within a specified radius. Entire clusters are then assigned to train or test to meet the split fraction.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Spatial or similarity-based data where locality grouping matters.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Intuitive radius control over cluster sizes.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Sensitive to radius choice; can produce imbalanced clusters.","category":"page"},{"location":"04-algorithms-overview/#Cluster-Shuffle-Split","page":"04. Algorithms Overview","title":"Cluster Shuffle Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Shuffles cluster labels and sequentially collects whole clusters into the training set until reaching the specified fraction, preserving inherent group structure.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Grouped or clustered data where splitting within groups is undesirable.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Maintains group integrity; introduces randomness.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"May overshoot or undershoot desired fraction if cluster sizes vary.","category":"page"},{"location":"04-algorithms-overview/#TargetPropertySplit","page":"04. Algorithms Overview","title":"TargetPropertySplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Partitions data into train/test sets by sorting samples according to a user-specified property (e.g., a column, a function of the sample, or a target value). The order argument (or alias) controls whether the largest/smallest property values are placed in the training set.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Useful for extrapolation or interpolation splits, e.g., when you want to train on the lowest (or highest) values of a property and test on the rest.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Simple and interpretable\nWorks with any property (target, feature, etc.)\nFlexible via property function and order aliases","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Not diversity-based; may not represent the full data distribution","category":"page"},{"location":"04-algorithms-overview/#TimeSplit","page":"04. Algorithms Overview","title":"TimeSplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Description: Splits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Use Cases: Useful for time series or temporal data where you want to avoid splitting samples with the same timestamp.","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Respects temporal order\nNever splits samples with the same date/time","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"04. Algorithms Overview","title":"04. Algorithms Overview","text":"Fraction may not be exact due to grouping","category":"page"},{"location":"#DataSplits","page":"DataSplits","title":"DataSplits","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Documentation for DataSplits.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"A tiny Julia library for rational train/test splitting algorithms:","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Strategy Purpose Complexity\nKennardStoneSplit Maximin split on X O(N²) time, O(N²) memory\nLazyKennardStoneSplit Same, streamed O(N²) time, O(N) mem\nSPXYSplit Joint X–y maximin (SPXY) O(N²) time, O(N²) mem\nOptiSimSplit Optimisable dissimilarity-based splitting O(N²) time, O(N²) memory\nMinimumDissimilaritySplit Greedy dissimilarity with one candidate O(N²) time, O(N²) memory\nMaximumDissimilaritySplit Greedy dissimilarity with full pool O(N²) time, O(N²) memory\nClusterShuffleSplit Cluster-based shuffle split O(N²) time, O(N²) memory","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"All splitting strategies in DataSplits are designed to work with any AbstractArray, including those with non-standard axes. This is achieved by mapping user indices to internal positions, ensuring correctness and extensibility for all data types.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"julia> using DataSplits, Distances\n\njulia> train, test = split(X, KennardStoneSplit(0.8))\njulia> train, test = split((X, y), SPXYSplit(0.7; metric = Cityblock()))","category":"page"}]
}
