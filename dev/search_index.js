var documenterSearchIndex = {"docs":
[{"location":"07-kennard-stone/#Kennard–Stone-Split","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"","category":"section"},{"location":"07-kennard-stone/#Overview","page":"Kennard–Stone Split","title":"Overview","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"The Kennard–Stone algorithm (also known as CADEX in some literature) is a deterministic method for selecting a representative training set from a dataset. It iteratively chooses the sample that is farthest (in feature space) from all previously selected samples, starting from the most distant pair. This ensures that the training set covers the full range of the feature space, making it especially useful for rational dataset splitting.","category":"page"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"CADEX stands for Computer Aided Design of Experiments and is an alias for the Kennard–Stone algorithm in DataSplits.","category":"page"},{"location":"07-kennard-stone/#How-it-works","page":"Kennard–Stone Split","title":"How it works","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"Compute the pairwise distance matrix for all samples.\nSelect the two samples that are farthest apart as the initial training set.\nIteratively add the sample that is farthest from the current training set (i.e., has the largest minimum distance to any selected sample).\nContinue until the desired number of training samples is reached.","category":"page"},{"location":"07-kennard-stone/#Arguments","page":"Kennard–Stone Split","title":"Arguments","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"frac: Fraction of samples to use for training (0 < frac < 1)\nmetric: Distance metric (default: Euclidean)","category":"page"},{"location":"07-kennard-stone/#Usage","page":"Kennard–Stone Split","title":"Usage","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"using DataSplits, Distances\nsplitter = KennardStoneSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"07-kennard-stone/#API-Reference","page":"Kennard–Stone Split","title":"API Reference","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"KennardStoneSplit\nLazyKennardStoneSplit\nsplit\nsplitdata","category":"page"},{"location":"07-kennard-stone/#References","page":"Kennard–Stone Split","title":"References","text":"","category":"section"},{"location":"07-kennard-stone/","page":"Kennard–Stone Split","title":"Kennard–Stone Split","text":"Kennard, R. W.; Stone, L. A. Computer Aided Design of Experiments. Technometrics 1969, 11 (1), 137–148. https://doi.org/10.1080/00401706.1969.10490666.","category":"page"},{"location":"09-spxy/#SPXY-Split","page":"SPXY Split","title":"SPXY Split","text":"","category":"section"},{"location":"09-spxy/#Overview","page":"SPXY Split","title":"Overview","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"The SPXY algorithm extends Kennard–Stone by considering both the feature matrix (X) and the target vector (y) when splitting data. It constructs a joint distance matrix as the sum of normalized pairwise distances in X and y, then applies the maximin selection. This ensures the training set is diverse in both predictors and response, which is especially important for regression tasks where the target distribution matters.","category":"page"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"MDKSSplit(frac): Alias for SPXYSplit(frac; metric=Mahalanobis()) (SPXY algorithm using Mahalanobis distance).","category":"page"},{"location":"09-spxy/#How-it-works","page":"SPXY Split","title":"How it works","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Compute the normalized pairwise distance matrix for X (features).\nCompute the normalized pairwise distance matrix for y (target).\nAdd the two matrices to form a joint distance matrix.\nApply the Kennard–Stone maximin selection on the joint distance matrix to select a representative training set.","category":"page"},{"location":"09-spxy/#Usage","page":"SPXY Split","title":"Usage","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"using DataSplits, Distances\nsplitter = SPXYSplit(0.7; metric=Cityblock())\nresult = split((X, y), splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"09-spxy/#Notes/Limitations","page":"SPXY Split","title":"Notes/Limitations","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Most appropriate for regression and continuous targets\nYou must call split((X, y), strategy) or split(X, y, strategy); calling split(X, strategy) will error","category":"page"},{"location":"09-spxy/#API-Reference","page":"SPXY Split","title":"API Reference","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"SPXYSplit\nMDKSSplit\nsplit\nsplitdata","category":"page"},{"location":"09-spxy/#References","page":"SPXY Split","title":"References","text":"","category":"section"},{"location":"09-spxy/","page":"SPXY Split","title":"SPXY Split","text":"Galvao, R.; Araujo, M.; Jose, G.; Pontes, M.; Silva, E.; Saldanha, T. A Method for Calibration and Validation Subset Partitioning. Talanta 2005, 67 (4), 736–740. https://doi.org/10.1016/j.talanta.2005.03.025. Saptoro, A.; Tadé, M. O.; Vuthaluru, H. A Modified Kennard-Stone Algorithm for Optimal Division of Data for Developing Artificial Neural Network Models. Chemical Product and Process Modeling 2012, 7 (1). https://doi.org/10.1515/1934-2659.1645.","category":"page"},{"location":"08-morais-lima-martin/#Morais–Lima–Martin-Split","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"","category":"section"},{"location":"08-morais-lima-martin/#Overview","page":"Morais–Lima–Martin Split","title":"Overview","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"The Morais–Lima–Martin algorithm performs a Kennard–Stone split, then randomly swaps a fraction of samples between the training and test sets. This introduces additional randomness to balance representativeness and variability.","category":"page"},{"location":"08-morais-lima-martin/#How-it-works","page":"Morais–Lima–Martin Split","title":"How it works","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"Apply Kennard–Stone to select the initial training set.\nRandomly select a fraction of samples from both train and test sets.\nSwap these selected samples between the two sets.","category":"page"},{"location":"08-morais-lima-martin/#Arguments","page":"Morais–Lima–Martin Split","title":"Arguments","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"frac: Fraction of samples to use for training (0 < frac < 1)\nswap_frac: Fraction of samples to swap between train and test (0 < swap_frac < 1)\nmetric: Distance metric to use for Kennard–Stone (default: Euclidean)","category":"page"},{"location":"08-morais-lima-martin/#Usage","page":"Morais–Lima–Martin Split","title":"Usage","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"using DataSplits, Distances\nsplitter = MoraisLimaMartinSplit(0.8; swap_frac=0.1, metric=Cityblock())\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"08-morais-lima-martin/#Reference","page":"Morais–Lima–Martin Split","title":"Reference","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"Morais, C. L. M.; Santos, M. C. D.; Lima, K. M. G.; Martin, F. L. Improving Data Splitting for Classification Applications in Spectrochemical Analyses Employing a Random-Mutation Kennard-Stone Algorithm Approach. Bioinformatics 2019, 35 (24), 5257–5263. https://doi.org/10.1093/bioinformatics/btz421.","category":"page"},{"location":"08-morais-lima-martin/#API-Reference","page":"Morais–Lima–Martin Split","title":"API Reference","text":"","category":"section"},{"location":"08-morais-lima-martin/","page":"Morais–Lima–Martin Split","title":"Morais–Lima–Martin Split","text":"MoraisLimaMartinSplit\nsplit\nsplitdata","category":"page"},{"location":"06-examples-tutorials/#Examples-and-Tutorials","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"Note: DataSplits expects data matrices to be in the Julia ML convention: columns are samples, rows are features. If your data uses rows as samples, transpose it before splitting (e.g., use X').","category":"page"},{"location":"06-examples-tutorials/#Example:-SPXY-Split-on-Custom-Data","page":"Examples & Tutorials","title":"Example: SPXY Split on Custom Data","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"using DataSplits, Distances\n# Suppose `df` is a DataFrame with features and target\ndf = DataFrame(rand(100, 5), :auto)\ny = rand(100)\nsplitter = SPXYSplit(0.75)\nresult = split((Matrix(df), y), splitter)\nX_train, X_test = splitdata(result, Matrix(df))","category":"page"},{"location":"06-examples-tutorials/#Example:-Custom-Splitter-Implementation","page":"Examples & Tutorials","title":"Example: Custom Splitter Implementation","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"Suppose you want to create a splitter that always assigns the first 80% of samples to train:","category":"page"},{"location":"06-examples-tutorials/","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"struct First80Split <: SplitStrategy end\n\nfunction _split(data, ::First80Split; rng=nothing)\n    N = numobs(data)\n    cut = floor(Int, 0.8 * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return TrainTestSplit(train_pos, test_pos)\nend","category":"page"},{"location":"06-examples-tutorials/#Example:-Group-aware-Splitting","page":"Examples & Tutorials","title":"Example: Group-aware Splitting","text":"","category":"section"},{"location":"06-examples-tutorials/","page":"Examples & Tutorials","title":"Examples & Tutorials","text":"using DataSplits, Clustering\nclusters = sphere_exclusion(X; radius=0.3)\nsplitter = ClusterShuffleSplit(clusters, 0.7)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"10-optisim/#OptiSim-Split","page":"OptiSim Split","title":"OptiSim Split","text":"","category":"section"},{"location":"10-optisim/#Overview","page":"OptiSim Split","title":"Overview","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"OptiSim is an iterative, optimisable splitting algorithm that aims to maximize the diversity of the training set by minimizing within-set similarity. At each step, it generates a temporary subsample of candidates and selects the one most dissimilar to the current training set, repeating until the desired fraction is reached. The algorithm is flexible and can be tuned for speed or quality by adjusting the subsample size and distance cutoff.","category":"page"},{"location":"10-optisim/#How-it-works","page":"OptiSim Split","title":"How it works","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"Compute the pairwise distance matrix for all samples.\nStart with a random sample in the training set.\nAt each iteration, generate a candidate subsample.\nSelect the candidate most dissimilar to the current training set.\nRepeat until the desired number of training samples is reached.","category":"page"},{"location":"10-optisim/#Arguments","page":"OptiSim Split","title":"Arguments","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"frac: Fraction of samples to use for training (0 < frac < 1)\nmax_subsample_size: Size of candidate pool at each step (default: 0, i.e., use all)\ndistance_cutoff: Threshold for similarity filtering (default: 0.35)\nmetric: Distance metric (default: Euclidean)","category":"page"},{"location":"10-optisim/#Usage","page":"OptiSim Split","title":"Usage","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"using DataSplits\nsplitter = OptiSimSplit(0.8; max_subsample_size=50)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"10-optisim/#Notes/Limitations","page":"OptiSim Split","title":"Notes/Limitations","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"Depends on the first selection, which is random\nMinimumDissimilaritySplit is an alias for OptiSimSplit with max_subsample_size=1. This provides a fast, greedy variant for large datasets where speed is more important than optimal diversity.\nMaximumDissimilaritySplit is an alias for OptiSimSplit with max_subsample_size=N. This yields a greedy, diversity-maximizing split. Note: This implementation does not discard the first two samples as in the original Maximum Dissimilarity algorithm, but otherwise follows the same greedy logic. This algorithm is known to greedily include outliers.","category":"page"},{"location":"10-optisim/#API-Reference","page":"OptiSim Split","title":"API Reference","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"OptiSimSplit\nsplit\nsplitdata","category":"page"},{"location":"10-optisim/#References","page":"OptiSim Split","title":"References","text":"","category":"section"},{"location":"10-optisim/","page":"OptiSim Split","title":"OptiSim Split","text":"Clark, R. D. OptiSim:  An Extended Dissimilarity Selection Method for Finding Diverse Representative Subsets. J. Chem. Inf. Comput. Sci. 1997, 37 (6), 1181–1188. https://doi.org/10.1021/ci970282v.","category":"page"},{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#DataSplits.ClusterShuffleSplit","page":"Reference","title":"DataSplits.ClusterShuffleSplit","text":"ClusterShuffleSplit(res::ClusteringResult, frac::Real)\nClusterShuffleSplit(f::Function, frac::Real, data)\n\nGroup-aware train/test splitting strategy that accumulates whole clusters in the training set until the requested fraction is reached.\n\nFields\n\nclusters::ClusteringResult: Clustering assignments for the data.\nfrac::ValidFraction{T}: Fraction of samples in the training set (0 < frac < 1).\n\nNotes\n\nThe clusters are accorpated to the training set in a random fashion. As such it is possible to overshoot the requested fraction. This algorithm does not attempt to obtain the split most similar to the requested fraction possible.\n\nExamples\n\nsplitter = ClusterShuffleSplit(clustering_result, 0.7)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.ClusterStratifiedSplit","page":"Reference","title":"DataSplits.ClusterStratifiedSplit","text":"ClusterStratifiedSplit(res::ClusteringResult, allocation::Symbol; n=nothing, frac)\nClusterStratifiedSplit(f::Function, allocation::Symbol; n=nothing, frac, data)\n\nCluster-stratified train/test splitting strategy with flexible allocation methods.\n\nFields\n\nclusters::ClusteringResult: Clustering assignments for the data.\nallocation::Symbol: Allocation method (:equal, :proportional, or :neyman).\nn::Union{Nothing,Int}: Number of samples per cluster (for :equal/:neyman).\nfrac::Real: Fraction of selected samples to use for training (rest go to test).\n\nExamples\n\nsplitter = ClusterStratifiedSplit(clustering_result, :proportional; frac=0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n# Reference\n\nFor a complete analysis of several methodologies refer to:\n\nMay, R. J.; Maier, H. R.; Dandy, G. C. Data Splitting for Artificial Neural Networks Using SOM-Based Stratified Sampling. Neural Networks 2010, 23 (2), 283–294. https://doi.org/10.1016/j.neunet.2009.11.009.\n\nThe implementation of equal is different than the one introduced in the paper and in previous references.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.CrossValidationSplit","page":"Reference","title":"DataSplits.CrossValidationSplit","text":"CrossValidationSplit\n\nA result type representing a k-fold cross-validation split.\n\nFields\n\nfolds::Vector{<:SplitResult}: Vector of TrainTestSplit or TrainValTestSplit, one per fold.\n\nNotes\n\nEach fold contains indices in the range 1:N, where N is the number of samples.\nFor custom data types, use getobs(X, indices) to access split data.\nData matrices are expected to have columns as samples (features × samples).\n\nExamples\n\nresult = split(X, SomeCVSplit(...))\nfor (X_train, X_test) in splitdata(result, X)\n    # ...\nend\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.KennardStoneSplit","page":"Reference","title":"DataSplits.KennardStoneSplit","text":"KennardStoneSplit{T} <: SplitStrategy\n\nSplitting strategy implementing the Kennard-Stone algorithm for train/test splitting.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric to use (default: Euclidean())\n\nExamples\n\nsplitter = KennardStoneSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n# Custom metric\nusing Distances\nsplitter = KennardStoneSplit(0.7, Cityblock())\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.LazyKennardStoneSplit","page":"Reference","title":"DataSplits.LazyKennardStoneSplit","text":"LazyKennardStoneSplit{T} <: SplitStrategy\n\nMemory-efficient Kennard-Stone (CADEX) splitting strategy for large datasets.\n\nPerforms train/test splitting using the maximin strategy, but avoids storing the full NxN distance matrix in memory (O(N) storage).\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric to use (default: Euclidean())\n\nExamples\n\nsplitter = LazyKennardStoneSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.MoraisLimaMartinSplit","page":"Reference","title":"DataSplits.MoraisLimaMartinSplit","text":"MoraisLimaMartinSplit(frac; swap_frac=0.1, metric=Euclidean())\n\nSplitting strategy executing Kennard–Stone then randomly swapping a fraction of samples between train and test sets.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\nswap_frac::ValidFraction{T}: Fraction of samples to swap between train and test (0 < swap_frac < 1)\nmetric::Distances.SemiMetric: Distance metric for Kennard–Stone (default: Euclidean())\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.OptiSimSplit","page":"Reference","title":"DataSplits.OptiSimSplit","text":"OptiSimSplit(frac; max_subsample_size, distance_cutoff = 0.10, metric = Euclidean())\n\nImplementation of the OptiSim (Clark 1997) optimizable K-dissimilarity selection strategy for train/test splitting.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of samples to return in the training subset (0 < frac < 1)\nmax_subsample_size::Integer: Size of the temporary sub-sample (default: max(1, ceil(Int, 0.05N)))\ndistance_cutoff::Real: Two points are “similar” if their distance < distance_cutoff (default: 0.10)\nmetric::Distances.SemiMetric: Distance metric (default: Euclidean())\n\nReferences\n\nClark, R. D. (1997). OptiSim: An Extended Dissimilarity Selection Method for Finding Diverse Representative Subsets. J. Chem. Inf. Comput. Sci., 37(6), 1181–1188.\n\nExamples\n\nsplitter = OptiSimSplit(0.7; max_subsample_size=10)\nresult = split(X, y, splitter)\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.RandomSplit","page":"Reference","title":"DataSplits.RandomSplit","text":"RandomSplit{T} <: SplitStrategy\n\nRandomly splits data into train/test sets according to the specified fraction.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\n\nExamples\n\nsplitter = RandomSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SPXYSplit-Tuple{Real}","page":"Reference","title":"DataSplits.SPXYSplit","text":"SPXYSplit(frac; metric = Euclidean())\n\nSample set Partitioning based on joint X–Y distance (SPXY).\n\nCreates an SPXY splitter, a variant of Kennard–Stone in which the distance matrix is the element-wise sum of:\n\nthe (normalized) pairwise distance matrix of the feature matrix X\nplus the (normalized) pairwise distance matrix of the response vector y.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of samples in the training subset (0 < frac < 1)\nmetric::Distances.SemiMetric: Distance metric used for both X and y (default: Euclidean())\n\nNotes\n\nsplit must be called with a 2-tuple (X, y) or with positional arguments split(X, y, strategy); calling split(X, strategy) will raise a MethodError.\n\nExamples\n\nsplitter = SPXYSplit(0.7)\nresult = split(X, y, splitter)\nX_train, X_test = splitdata(result, X)\n\nSee also\n\nKennardStoneSplit — the classical variant that uses only X.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.SphereExclusionResult","page":"Reference","title":"DataSplits.SphereExclusionResult","text":"SphereExclusionResult\n\nResult of sphere exclusion clustering.\n\nFields\n\nassignments::Vector{Int}: Cluster index per point (1-based).\nradius::Float64: Exclusion radius used for clustering.\nmetric::Distances.SemiMetric: Distance metric used.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SplitInputError","page":"Reference","title":"DataSplits.SplitInputError","text":"SplitInputError(msg)\n\nError thrown when input data to a split is invalid (e.g., empty, wrong shape, mismatched X/y).\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SplitNotImplementedError","page":"Reference","title":"DataSplits.SplitNotImplementedError","text":"SplitNotImplementedError(msg)\n\nError thrown when a required split method or feature is not implemented.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SplitParameterError","page":"Reference","title":"DataSplits.SplitParameterError","text":"SplitParameterError(msg)\n\nError thrown when split parameters are invalid (e.g., unknown allocation, out-of-bounds fraction).\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.SplitResult","page":"Reference","title":"DataSplits.SplitResult","text":"SplitResult\n\nAbstract supertype for all split result types.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TargetPropertySplit","page":"Reference","title":"DataSplits.TargetPropertySplit","text":"TargetPropertySplit{T} <: SplitStrategy\n\nSplits a 1D property array into train/test sets by sorting the property values.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc, :desc, :high, :low, :largest, :smallest, etc.\n\nReturns\n\nTrainTestSplit: Indices for train and test sets.\n\nExamples\n\nsplitter = TargetPropertyHigh(0.8)\nresult = split(y, splitter)\ntrain_idx, test_idx = splitdata(result, y)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TimeSplit","page":"Reference","title":"DataSplits.TimeSplit","text":"TimeSplit{T} <: SplitStrategy\n\nSplits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.\n\nFields\n\nfrac::ValidFraction{T}: Fraction of data to use for training (0 < frac < 1)\norder::Symbol: Sorting order; use :asc (oldest in train, default), :desc (newest in train)\n\nReturns\n\nTrainTestSplit: Indices for train and test sets.\n\nExamples\n\nsplitter = TimeSplitOldest(0.7)\nresult = split(dates, splitter)\ntrain_idx, test_idx = splitdata(result, dates)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TrainTestSplit","page":"Reference","title":"DataSplits.TrainTestSplit","text":"TrainTestSplit\n\nA result type representing a train/test split.\n\nFields\n\ntrain::Vector{Int}: Indices (1:N) of training samples.\ntest::Vector{Int}: Indices (1:N) of test samples.\n\nNotes\n\nIndices are always in the range 1:N, where N is the number of samples.\nFor custom data types, use getobs(X, indices) to access split data.\nData matrices are expected to have columns as samples (features × samples).\n\nExamples\n\nresult = split(X, KennardStoneSplit(0.8))\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.TrainValTestSplit","page":"Reference","title":"DataSplits.TrainValTestSplit","text":"TrainValTestSplit\n\nA result type representing a train/validation/test split.\n\nFields\n\ntrain::Vector{Int}: Indices (1:N) of training samples.\nval::Vector{Int}: Indices (1:N) of validation samples.\ntest::Vector{Int}: Indices (1:N) of test samples.\n\nNotes\n\nIndices are always in the range 1:N, where N is the number of samples.\nFor custom data types, use getobs(X, indices) to access split data.\nData matrices are expected to have columns as samples (features × samples).\n\nExamples\n\nresult = split(X, SomeTrainValTestSplit(...))\nX_train, X_val, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#DataSplits.MDKSSplit-Tuple{Real}","page":"Reference","title":"DataSplits.MDKSSplit","text":"MDKSSplit(frac::Real)\n\nAlias for SPXYSplit(frac; metric = Mahalanobis()). Uses the Mahalanobis distance for both X and y.\n\nArguments\n\nfrac: Fraction of samples to use for training (0 < frac < 1)\n\nExamples\n\nsplitter = MDKSSplit(0.7)\nresult = split((X, y), splitter)\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{AbstractArray, AbstractVector, SPXYSplit}","page":"Reference","title":"DataSplits._split","text":"_split(X, y, strategy::SPXYSplit; rng = Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nSplit a feature matrix X and response vector y into train/test subsets using the SPXY algorithm:\n\nBuild the joint distance matrix D = D_X + D_Y  (see SPXYSplit for details).\nRun the Kennard–Stone maximin procedure on D.\nReturn two sorted index vectors (train_idx, test_idx).\n\nArguments\n\nname type requirement\nX AbstractMatrix size(X, 1) == length(y)\ny AbstractVector \nstrategy SPXYSplit created with SPXYSplit(frac; metric)\nrng random‑number source unused here but kept for API symmetry\n\nReturns\n\nTwo Vector{Int} with the sample indices of X (and the corresponding entries of y) that belong to the training and test subsets.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, ClusterStratifiedSplit}","page":"Reference","title":"DataSplits._split","text":"cluster_stratified(N, s, rng, data)\n\nMain splitting function. For each cluster, selects indices according to the allocation method, then splits those indices into train/test according to frac.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, KennardStoneSplit}","page":"Reference","title":"DataSplits._split","text":"_split(data, s::KennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nOptimized in-memory Kennard-Stone algorithm using precomputed distance matrix. Best for small-to-medium datasets where O(N²) memory is acceptable.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, LazyKennardStoneSplit}","page":"Reference","title":"DataSplits._split","text":"_split(data, s::LazyKennardStoneSplit; rng=Random.GLOBAL_RNG) → (train_idx, test_idx)\n\nKennard-Stone (CADEX) algorithm for optimal train/test splitting using maximin strategy. Memory-optimized implementation with O(N) storage. Useful when working with large datasets where the NxN distance matrix does not fit memory. When working with small datasets, use the traditional implementation.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits._split-Tuple{Any, TargetPropertySplit}","page":"Reference","title":"DataSplits._split","text":"targetpropertysplit(N, s, rng, data)\n\nSorts the property array and splits into train/test according to s.frac and s.order.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.distance_matrix-Tuple{Any, Distances.PreMetric}","page":"Reference","title":"DataSplits.distance_matrix","text":"distance_matrix(X, metric::PreMetric)\n\nComputes the full symmetric pairwise distance matrix for the dataset X using the given metric.\n\nArguments\n\nX: Data matrix or container. Columns are samples (features × samples).\nmetric::PreMetric: Distance metric from Distances.jl.\n\nReturns\n\nD::Matrix{Float64}: Matrix where D[i, j] = metric(xᵢ, xⱼ) and D[i, j] == D[j, i].\n\nNotes\n\nFor custom containers, getobs(X, i) is used to access samples.\nThe matrix is symmetric and not normalized.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.equal_allocation-NTuple{4, Any}","page":"Reference","title":"DataSplits.equal_allocation","text":"equal_allocation(cl_ids, idxs_by_cluster, n, rng)\n\nRandomly select n samples from each cluster (or all if cluster is smaller). Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_maximin_element-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Union{AbstractSet{Int64}, AbstractVector{Int64}}, Union{AbstractSet{Int64}, AbstractVector{Int64}}}} where T<:Real","page":"Reference","title":"DataSplits.find_maximin_element","text":"find_maximin_element(distances::AbstractMatrix{T},\n                    source_set::Union{AbstractVector{Int},AbstractSet{Int}},\n                    reference_set::Union{AbstractVector{Int},AbstractSet{Int}}) -> Int\n\nFinds the element in source_set that maximizes the minimum distance to all elements in reference_set.\n\nArguments\n\ndistances::AbstractMatrix{T}: Precomputed, symmetric pairwise distance matrix (N×N).\nsource_set::Union{AbstractVector{Int},AbstractSet{Int}}: Indices to evaluate.\nreference_set::Union{AbstractVector{Int},AbstractSet{Int}}: Indices to compare against.\n\nReturns\n\nInt: Index in source_set that is farthest from its nearest neighbor in reference_set.\n\nNotes\n\nThrows ArgumentError if reference_set is empty.\nBreaks ties by returning the first maximum.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.find_most_distant_pair-Tuple{AbstractMatrix}","page":"Reference","title":"DataSplits.find_most_distant_pair","text":"find_most_distant_pair(D::AbstractMatrix) → (i, j)\n\nFinds indices of most distant pair in precomputed distance matrix.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.neyman_allocation-NTuple{5, Any}","page":"Reference","title":"DataSplits.neyman_allocation","text":"neyman_allocation(cl_ids, idxs_by_cluster, n, data, rng)\n\nRandomly select a Neyman quota from each cluster (proportional to cluster size and mean std of features). Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.proportional_allocation-Tuple{Any, Any, Any}","page":"Reference","title":"DataSplits.proportional_allocation","text":"proportional_allocation(cl_ids, idxs_by_cluster, rng)\n\nUse all samples in each cluster, shuffled. Returns a Dict mapping cluster id to selected indices.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.sphere_exclusion-Tuple{Any}","page":"Reference","title":"DataSplits.sphere_exclusion","text":"sphere_exclusion(data; radius::Real, metric::Distances.SemiMetric=Euclidean()) -> SphereExclusionResult\n\nClusters samples in data using the sphere exclusion algorithm.\n\nArguments\n\ndata: Data matrix or container. Columns are samples.\nradius::Real: Exclusion radius (normalized to [0, 1]).\nmetric::Distances.SemiMetric: Distance metric (default: Euclidean()).\n\nReturns\n\nSphereExclusionResult: Clustering result with assignments, radius, and metric.\n\nNotes\n\nThe distance matrix is normalized to [0, 1] before clustering.\nEach cluster contains all points within radius of the cluster center.\n\nExamples\n\nresult = sphere_exclusion(X; radius=0.2)\nassignments = result.assignments\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.split-Tuple{Any, SplitStrategy}","page":"Reference","title":"DataSplits.split","text":"split(data, strategy; rng=Random.default_rng()) -> SplitResult\n\nSplit data into train/test (or train/val/test, or cross-validation) sets according to the given strategy.\n\nArguments\n\ndata: Data matrix (columns are samples) or custom container.\nstrategy::SplitStrategy: The splitting strategy to use.\nrng: Optional random number generator.\n\nReturns\n\nSplitResult: An object containing the split indices.\n\nNotes\n\nAll returned indices are in the range 1:N, where N is the number of samples.\nFor custom data types, implement Base.length and Base.getindex as per MLUtils.\n\nExamples\n\nresult = split(X, KennardStoneSplit(0.8))\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.splitdata-Tuple{SplitResult, Any}","page":"Reference","title":"DataSplits.splitdata","text":"splitdata(result::SplitResult, X)\n\nReturn the train/test (and optionally validation) splits from a SplitResult for the given data.\n\nArguments\n\nresult::SplitResult: The result of a splitting strategy.\nX: Data matrix or custom container. Columns are samples.\n\nReturns\n\nTuple of data splits, e.g. (X_train, X_test).\n\nNotes\n\nAll indices in result are in the range 1:N, where N is the number of samples.\nFor custom data types, use getobs(X, indices) to access split data.\nData matrices are expected to have columns as samples (features × samples).\n\nExamples\n\nresult = split(X, KennardStoneSplit(0.8))\nX_train, X_test = splitdata(result, X)\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#DataSplits.train_test_counts-Tuple{Integer, Any}","page":"Reference","title":"DataSplits.train_test_counts","text":"train_test_counts(N, frac; min_train=2, min_test=2)\n\nGiven total sample count N and train fraction frac, return (n_train, n_test). Throws SplitParameterError if the split is not possible (e.g., too few samples, fraction out of bounds).\n\n\n\n\n\n","category":"method"},{"location":"03-core-api-reference/#Core-API-Reference","page":"Core API Reference","title":"Core API Reference","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"Note: DataSplits expects data matrices to follow the Julia ML convention: columns are samples, rows are features. All examples and API references below assume this convention. If you have data with samples as rows, transpose it before splitting (e.g., use X').","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"For custom data types, implement Base.length (number of samples) and Base.getindex(data, i) (returning the i-th sample) as described in the MLUtils documentation. This ensures compatibility with all DataSplits algorithms and utilities.","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"All split strategies in DataSplits return indices in the range 1:N (where N is the number of samples). For non-standard arrays or custom containers, always use getobs(X, indices) to access the split data, as this will correctly handle any custom indexing or axes.","category":"page"},{"location":"03-core-api-reference/#split","page":"Core API Reference","title":"split","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"split(data, strategy; rng)","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"Dispatches to strategy._split. Returns a SplitResult object representing the split.","category":"page"},{"location":"03-core-api-reference/#SplitResult-API","page":"Core API Reference","title":"SplitResult API","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"The result of a split is always a subtype of the abstract type SplitResult. This API provides a structured, type-safe way to represent train/test (and validation) splits, as well as cross-validation folds.","category":"page"},{"location":"03-core-api-reference/#Abstract-Type","page":"Core API Reference","title":"Abstract Type","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"abstract type SplitResult end","category":"page"},{"location":"03-core-api-reference/#Concrete-Subtypes","page":"Core API Reference","title":"Concrete Subtypes","text":"","category":"section"},{"location":"03-core-api-reference/#TrainTestSplit","page":"Core API Reference","title":"TrainTestSplit","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"struct TrainTestSplit{I} <: SplitResult\n    train::I\n    test::I\nend","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"train: indices of training samples\ntest: indices of test samples","category":"page"},{"location":"03-core-api-reference/#TrainValTestSplit","page":"Core API Reference","title":"TrainValTestSplit","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"struct TrainValTestSplit{I} <: SplitResult\n    train::I\n    val::I\n    test::I\nend","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"train: indices of training samples\nval: indices of validation samples\ntest: indices of test samples","category":"page"},{"location":"03-core-api-reference/#CrossValidationSplit","page":"Core API Reference","title":"CrossValidationSplit","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"struct CrossValidationSplit{T<:SplitResult} <: SplitResult\n    folds::Vector{T}\nend","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"folds: a vector of TrainTestSplit or TrainValTestSplit objects, one per fold","category":"page"},{"location":"03-core-api-reference/#splitdata","page":"Core API Reference","title":"splitdata","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"splitdata(result::SplitResult, X)","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"Given a SplitResult and data X, returns the corresponding splits as a tuple. For example:","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"For TrainTestSplit, returns (X_train, X_test)\nFor TrainValTestSplit, returns (X_train, X_val, X_test)\nFor CrossValidationSplit, returns a vector of tuples, one per fold","category":"page"},{"location":"03-core-api-reference/#Example-Usage","page":"Core API Reference","title":"Example Usage","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"using DataSplits\n\nresult = split(X, KennardStoneSplit(0.8))\nX_train, X_test = splitdata(result, X)\n\nresult = split(X, ClusterStratifiedSplit(clusters, :equal; n=4, frac=0.7))\nX_train, X_test = splitdata(result, X)\n\n# Cross-validation\ncv_result = split(X, SomeCVSplit(...))\nfor (X_train, X_test) in splitdata(cv_result, X)\n    # ...\nend","category":"page"},{"location":"03-core-api-reference/#Indices-returned-by-split-strategies","page":"Core API Reference","title":"Indices returned by split strategies","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"All split strategies in DataSplits return indices in the range 1:N (where N is the number of samples). For non-standard arrays or custom containers, always use getobs(X, indices) to access the split data, as this will correctly handle any custom indexing or axes. This approach is fully compatible with the MLUtils interface and ensures robust, generic code for all data types.","category":"page"},{"location":"03-core-api-reference/#SplitStrategy-Interface","page":"Core API Reference","title":"SplitStrategy Interface","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"To add a new strategy, subtype SplitStrategy and implement:","category":"page"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"_split(data, s::YourStrategy; rng)","category":"page"},{"location":"03-core-api-reference/#Utility-Functions","page":"Core API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"sample_indices(data): returns iterable indices of samples (default: 1:size(data,1), but now robust to any AbstractArray axes).\nValidFraction: bounds-checked fraction type for split ratios.","category":"page"},{"location":"03-core-api-reference/#Example:-Custom-Splitter-(new-pattern)","page":"Core API Reference","title":"Example: Custom Splitter (new pattern)","text":"","category":"section"},{"location":"03-core-api-reference/","page":"Core API Reference","title":"Core API Reference","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    N = numobs(data)\n    train_pos, test_pos = mysplit(N, s, rng, data)\n    return TrainTestSplit(train_pos, test_pos)\nend","category":"page"},{"location":"02-getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"02-getting-started/#Basic-API","page":"Getting Started","title":"Basic API","text":"","category":"section"},{"location":"02-getting-started/","page":"Getting Started","title":"Getting Started","text":"split(data, strategy): main entry point.\ndata: array, tuple (X,) or (X, y).\nstrategy: subtype of SplitStrategy.","category":"page"},{"location":"02-getting-started/#Data-Formats","page":"Getting Started","title":"Data Formats","text":"","category":"section"},{"location":"02-getting-started/","page":"Getting Started","title":"Getting Started","text":"Accepts matrices, arrays, tables, or custom types.\nImportant: DataSplits expects matrices to be in the Julia ML convention: columns are samples, rows are features. If your data uses rows as samples, transpose it before splitting (e.g., use X').\nFor custom data types, implement Base.length (number of samples) and Base.getindex(data, i) (returning the i-th sample) as described in the MLUtils documentation.","category":"page"},{"location":"02-getting-started/#Robust-Index-Handling","page":"Getting Started","title":"Robust Index Handling","text":"","category":"section"},{"location":"02-getting-started/","page":"Getting Started","title":"Getting Started","text":"All splitting strategies in DataSplits are robust to arrays with arbitrary axes (e.g., OffsetArrays, SubArrays, etc.). The library automatically handles mapping between user-facing indices and internal positions, so you can use any AbstractArray as input.","category":"page"},{"location":"02-getting-started/#Randomness-Control","page":"Getting Started","title":"Randomness Control","text":"","category":"section"},{"location":"02-getting-started/","page":"Getting Started","title":"Getting Started","text":"Pass rng keyword to strategies supporting it, e.g. split(X, RandomSplit(0.7); rng=123).","category":"page"},{"location":"02-getting-started/#Example:-Custom-Data-Type","page":"Getting Started","title":"Example: Custom Data Type","text":"","category":"section"},{"location":"02-getting-started/","page":"Getting Started","title":"Getting Started","text":"To use your own data type, implement sample_indices(data) and get_sample(data, i).","category":"page"},{"location":"13-sphere-exclusion/#Sphere-Exclusion-Split","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"","category":"section"},{"location":"13-sphere-exclusion/#Overview","page":"Sphere Exclusion Split","title":"Overview","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"Sphere Exclusion Split is a clustering-based splitting strategy. It works by iteratively picking an unassigned sample and forming a cluster of all points within a specified radius (in normalized distance). This process repeats until all samples are assigned to clusters. Clusters are then assigned to the training set until the desired fraction is reached. This method is especially useful for spatial or similarity-based data, where you want to avoid splitting local neighborhoods.","category":"page"},{"location":"13-sphere-exclusion/#When-to-Use","page":"Sphere Exclusion Split","title":"When to Use","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"For spatial, chemical, or biological data where locality matters\nWhen you want to avoid splitting similar samples across train/test","category":"page"},{"location":"13-sphere-exclusion/#When-Not-to-Use","page":"Sphere Exclusion Split","title":"When Not to Use","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"When you do not know how to set a meaningful radius\nFor data with no meaningful distance metric","category":"page"},{"location":"13-sphere-exclusion/#Arguments","page":"Sphere Exclusion Split","title":"Arguments","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"frac: Fraction of samples to use for training (0 < frac < 1)\nradius: Normalized distance threshold [0,1]\nmetric: Distance metric (default: Euclidean)","category":"page"},{"location":"13-sphere-exclusion/#Usage","page":"Sphere Exclusion Split","title":"Usage","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"using DataSplits\nsplitter = SphereExclusionSplit(0.7; radius=0.2)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"13-sphere-exclusion/#Notes/Limitations","page":"Sphere Exclusion Split","title":"Notes/Limitations","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"Sensitive to radius; cluster sizes may be uneven\nMay not work well for high-dimensional or non-metric data","category":"page"},{"location":"13-sphere-exclusion/#API-Reference","page":"Sphere Exclusion Split","title":"API Reference","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"sphere_exclusion\nsplit\nsplitdata","category":"page"},{"location":"13-sphere-exclusion/#References","page":"Sphere Exclusion Split","title":"References","text":"","category":"section"},{"location":"13-sphere-exclusion/","page":"Sphere Exclusion Split","title":"Sphere Exclusion Split","text":"Sphere Exclusion Clustering","category":"page"},{"location":"05-extending-data-splits/#Extending-DataSplits","page":"Extending DataSplits","title":"Extending DataSplits","text":"","category":"section"},{"location":"05-extending-data-splits/#Checklist-for-Adding-a-New-Splitter","page":"Extending DataSplits","title":"Checklist for Adding a New Splitter","text":"","category":"section"},{"location":"05-extending-data-splits/","page":"Extending DataSplits","title":"Extending DataSplits","text":"Subtype SplitStrategy.\nImplement _split(data, s; rng).\nUse ValidFraction for fraction validation.\nAdd a docstring and example usage.\nAdd a test in test/.","category":"page"},{"location":"05-extending-data-splits/","page":"Extending DataSplits","title":"Extending DataSplits","text":"Best Practices:","category":"page"},{"location":"05-extending-data-splits/","page":"Extending DataSplits","title":"Extending DataSplits","text":"Always validate input shapes and types.\nUse getobs and numobs for compatibility with custom data types.\nReturn a TrainTestSplit or other SplitResult subtype.\nDocument edge cases and limitations.","category":"page"},{"location":"05-extending-data-splits/","page":"Extending DataSplits","title":"Extending DataSplits","text":"Example:","category":"page"},{"location":"05-extending-data-splits/","page":"Extending DataSplits","title":"Extending DataSplits","text":"struct MySplit <: SplitStrategy; frac; end\n\nfunction mysplit(N, s, rng, data)\n    cut = floor(Int, s.frac * N)\n    train_pos = 1:cut\n    test_pos = (cut+1):N\n    return train_pos, test_pos\nend\n\nfunction _split(data, s::MySplit; rng=Random.default_rng())\n    N = numobs(data)\n    train_pos, test_pos = mysplit(N, s, rng, data)\n    return TrainTestSplit(train_pos, test_pos)\nend","category":"page"},{"location":"14-cluster-stratified/#Cluster-Stratified-Split","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"","category":"section"},{"location":"14-cluster-stratified/#Overview","page":"Cluster Stratified Split","title":"Overview","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"Cluster Stratified Split is a train/test splitting strategy that ensures each cluster (as determined by a clustering algorithm) is split into train and test sets according to a specified allocation method and user-defined train fraction.","category":"page"},{"location":"14-cluster-stratified/#Allocation-Methods","page":"Cluster Stratified Split","title":"Allocation Methods","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"Equal allocation: Randomly selects a fixed number n of samples from each cluster, then splits those into train/test according to the user-specified frac (fraction for train set). The rest of the cluster is unused.\nProportional allocation: Uses all samples in each cluster, splits them into train/test according to the user-specified frac (fraction for train set).\nNeyman allocation: Randomly selects a quota from each cluster (proportional to cluster size and mean standard deviation of features), then splits those into train/test according to the user-specified frac (fraction for train set). The rest of the cluster is unused.","category":"page"},{"location":"14-cluster-stratified/#When-to-Use","page":"Cluster Stratified Split","title":"When to Use","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"When you want to preserve cluster structure in both train and test sets\nFor grouped or clustered data where stratification is important","category":"page"},{"location":"14-cluster-stratified/#When-Not-to-Use","page":"Cluster Stratified Split","title":"When Not to Use","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"When clusters are very imbalanced in size (may affect stratification)\nFor unstructured data with no meaningful groups","category":"page"},{"location":"14-cluster-stratified/#Arguments","page":"Cluster Stratified Split","title":"Arguments","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"clusters: ClusteringResult (from Clustering.jl)\nallocation: :equal, :proportional, or :neyman\nn: Number of samples per cluster (for :equal and :neyman)\nfrac: Fraction of selected samples to use for train (rest go to test)","category":"page"},{"location":"14-cluster-stratified/#Usage","page":"Cluster Stratified Split","title":"Usage","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"using DataSplits, Clustering\nclusters = sphere_exclusion(X; radius=0.3)\nsplitter = ClusterStratifiedSplit(clusters, :proportional; frac=0.7)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"14-cluster-stratified/#Notes/Limitations","page":"Cluster Stratified Split","title":"Notes/Limitations","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"If n is greater than the cluster size, all samples in the cluster are used\nFor :proportional, all samples are always used\nFor frac=1.0, all selected samples go to train; for frac=0.0, all go to test\nThe split is performed per cluster; rounding is handled so that the train set always gets the larger share when the split is not exact","category":"page"},{"location":"14-cluster-stratified/#API-Reference","page":"Cluster Stratified Split","title":"API Reference","text":"","category":"section"},{"location":"14-cluster-stratified/","page":"Cluster Stratified Split","title":"Cluster Stratified Split","text":"ClusterStratifiedSplit\nsplit\nsplitdata","category":"page"},{"location":"01-introduction/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"01-introduction/#Motivation-and-Features","page":"Introduction","title":"Motivation and Features","text":"","category":"section"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"DataSplits provides rational train/test splitting algorithms and clustering-based pre-processing for reproducible model evaluation. Unlike random splits, these methods ensure that the training and test sets are representative and diverse, which is especially important for small or structured datasets.","category":"page"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"Key Features:","category":"page"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"Multiple splitting strategies: maximin, clustering-based, group-aware, and more\nExtensible API for custom strategies\nWorks with arrays, tuples, and custom data types","category":"page"},{"location":"01-introduction/#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"] add https://github.com/davide-grheco/DataSplits.jl","category":"page"},{"location":"01-introduction/#When-to-Use-DataSplits","page":"Introduction","title":"When to Use DataSplits","text":"","category":"section"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"When random splits are not enough (e.g., small datasets, strong structure)\nWhen you need reproducible, rational splits for benchmarking\nWhen you want to preserve group or cluster structure in your splits","category":"page"},{"location":"01-introduction/#Quickstart-Example","page":"Introduction","title":"Quickstart Example","text":"","category":"section"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"using DataSplits, Distances\n\n# Simple Kennard–Stone split\nsplitter = KennardStoneSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n# SPXY split on features and target\nsplitter = SPXYSplit(0.7; metric=Cityblock())\nresult = split((X, y), splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"01-introduction/#Glossary","page":"Introduction","title":"Glossary","text":"","category":"section"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"train/test: The two sets into which data is split for model training and evaluation.\nfraction: The proportion of samples to assign to the training set (between 0 and 1).\nindices: Integer positions of samples in the data array.\nsamples: Individual data points (columns in a matrix, or elements in a vector).\nsplitter/strategy: An object describing how to split the data (e.g., KennardStoneSplit).\nSplitResult: The object returned by split, containing train/test indices.\nsplitdata: Function to extract the actual data splits from a SplitResult.\nClusteringResult: Object representing cluster assignments for samples.","category":"page"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"01-introduction/","page":"Introduction","title":"Introduction","text":"For more details, see the Getting Started and Core API Reference.","category":"page"},{"location":"14-cluster-shuffle/#Cluster-Shuffle-Split","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"","category":"section"},{"location":"14-cluster-shuffle/#Overview","page":"Cluster Shuffle Split","title":"Overview","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"Cluster Shuffle Split is a group-aware splitting strategy. It either takes a precomputed clustering result or a function to generate one, then shuffles the cluster labels and accumulates whole clusters into the training set until the desired fraction is reached. This approach is ideal for grouped or clustered data where splitting within groups would break structure or introduce leakage.","category":"page"},{"location":"14-cluster-shuffle/#When-to-Use","page":"Cluster Shuffle Split","title":"When to Use","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"When your data has natural groups or clusters (e.g., patients, molecules, batches)\nWhen you want to avoid splitting groups across train/test","category":"page"},{"location":"14-cluster-shuffle/#When-Not-to-Use","page":"Cluster Shuffle Split","title":"When Not to Use","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"When clusters are very imbalanced in size (fraction control is coarse)\nFor unstructured data with no meaningful groups","category":"page"},{"location":"14-cluster-shuffle/#Arguments","page":"Cluster Shuffle Split","title":"Arguments","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"res or f,data: Clustering result or function\nfrac: Fraction of samples to use for training (0 < frac < 1)\nrng: Optional RNG","category":"page"},{"location":"14-cluster-shuffle/#Usage","page":"Cluster Shuffle Split","title":"Usage","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"using DataSplits, Clustering\nclusters = sphere_exclusion(X; radius=0.3)\nsplitter = ClusterShuffleSplit(clusters, 0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"14-cluster-shuffle/#Notes/Limitations","page":"Cluster Shuffle Split","title":"Notes/Limitations","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"Fraction control is coarse; may overshoot/undershoot target\nCluster sizes may vary widely","category":"page"},{"location":"14-cluster-shuffle/#API-Reference","page":"Cluster Shuffle Split","title":"API Reference","text":"","category":"section"},{"location":"14-cluster-shuffle/","page":"Cluster Shuffle Split","title":"Cluster Shuffle Split","text":"ClusterShuffleSplit\nsplit\nsplitdata","category":"page"},{"location":"04-algorithms-overview/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Each splitter returns two sets of indices (train, test), partitioning samples according to the chosen strategy. DataSplits expects data matrices to be in the Julia ML convention: columns are samples, rows are features. Choose based on dataset size, desired diversity, or grouping needs.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"For custom data types, implement Base.length (number of samples) and Base.getindex(data, i) (returning the i-th sample) as described in the MLUtils documentation.","category":"page"},{"location":"04-algorithms-overview/#Kennard–Stone-Split","page":"Algorithms","title":"Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Iteratively selects samples by choosing the point farthest from all previously chosen points in the feature space. This approach ensures the training set uniformly covers the distribution of predictors.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Ideal for small to medium datasets where uniform feature coverage improves model generalization.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Deterministic selection ensures reproducible splits.\nProvides diverse representation of feature space.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Requires storing and computing full distance matrix, which can be memory-intensive.","category":"page"},{"location":"04-algorithms-overview/#Lazy-Kennard–Stone-Split","page":"Algorithms","title":"Lazy Kennard–Stone Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: A streaming variant of Kennard–Stone that computes distances on the fly, avoiding the full matrix. Suitable when memory is constrained.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Large datasets where memory is a bottleneck.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Reduced memory overhead.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"May incur extra computational overhead due to repeated distance computations.","category":"page"},{"location":"04-algorithms-overview/#SPXY-Split","page":"Algorithms","title":"SPXY Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Extends Kennard–Stone to both features (X) and target variable (y), selecting samples that maximize combined spread. Ensures training set represents both predictors and response.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Regression tasks where preserving target distribution is critical.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Balances feature and response diversity.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Still requires pairwise distance computations; may overweight target variation.","category":"page"},{"location":"04-algorithms-overview/#OptiSim-Split","page":"Algorithms","title":"OptiSim Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Applies iterative swapping of samples between train and test sets to minimize within-set dissimilarity, guided by a dissimilarity measure.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: When split quality is paramount and computational resources allow.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Can yield high-quality, low-dissimilarity splits.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Computationally intensive; sensitive to initial split.","category":"page"},{"location":"04-algorithms-overview/#Minimum/Maximum-Dissimilarity-Split","page":"Algorithms","title":"Minimum/Maximum Dissimilarity Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Greedy cluster-based strategy that adds clusters to the training set based on minimal or maximal dissimilarity criteria until the desired fraction is reached.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: When a fast, cluster-aware splitting heuristic is sufficient.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Simpler and faster than full optimization.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Greedy nature may miss globally optimal arrangement; cluster sizes may vary.","category":"page"},{"location":"04-algorithms-overview/#Sphere-Exclusion-Split","page":"Algorithms","title":"Sphere Exclusion Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Forms clusters by selecting a sample and excluding all neighbors within a specified radius. Entire clusters are then assigned to train or test to meet the split fraction.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Spatial or similarity-based data where locality grouping matters.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Intuitive radius control over cluster sizes.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Sensitive to radius choice; can produce imbalanced clusters.","category":"page"},{"location":"04-algorithms-overview/#Cluster-Shuffle-Split","page":"Algorithms","title":"Cluster Shuffle Split","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Shuffles cluster labels and sequentially collects whole clusters into the training set until reaching the specified fraction, preserving inherent group structure.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Grouped or clustered data where splitting within groups is undesirable.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Maintains group integrity; introduces randomness.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"May overshoot or undershoot desired fraction if cluster sizes vary.","category":"page"},{"location":"04-algorithms-overview/#TargetPropertySplit","page":"Algorithms","title":"TargetPropertySplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Partitions data into train/test sets by sorting samples according to a user-specified property (e.g., a column, a function of the sample, or a target value). The order argument (or alias) controls whether the largest/smallest property values are placed in the training set.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Useful for extrapolation or interpolation splits, e.g., when you want to train on the lowest (or highest) values of a property and test on the rest.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Simple and interpretable\nWorks with any property (target, feature, etc.)\nFlexible via property function and order aliases","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Not diversity-based; may not represent the full data distribution","category":"page"},{"location":"04-algorithms-overview/#TimeSplit","page":"Algorithms","title":"TimeSplit","text":"","category":"section"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Description: Splits a 1D array of dates/times into train/test sets, grouping by unique date/time values. No group (samples with the same date) is split between train and test. The actual fraction may be slightly above the requested one, but never below.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Use Cases: Useful for time series or temporal data where you want to avoid splitting samples with the same timestamp.","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Pros:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Respects temporal order\nNever splits samples with the same date/time","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Cons:","category":"page"},{"location":"04-algorithms-overview/","page":"Algorithms","title":"Algorithms","text":"Fraction may not be exact due to grouping","category":"page"},{"location":"#DataSplits","page":"DataSplits","title":"DataSplits","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"DataSplits is a Julia library for rational train/test splitting algorithms. It provides a variety of strategies for splitting datasets. In several applications random selection is not an appropriate choice and may lead to overestimating model performance.","category":"page"},{"location":"#Quick-Start","page":"DataSplits","title":"Quick Start","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"using DataSplits, Distances\n\n# Kennard–Stone split (maximin)\nsplitter = KennardStoneSplit(0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)\n\n# SPXY split (joint X–y diversity)\nsplitter = SPXYSplit(0.7; metric=Cityblock())\nresult = split((X, y), splitter)\nX_train, X_test = splitdata(result, X)\n\n# Cluster-based split\nusing Clustering\nclusters = sphere_exclusion(X; radius=0.3)\nsplitter = ClusterShuffleSplit(clusters, 0.8)\nresult = split(X, splitter)\nX_train, X_test = splitdata(result, X)","category":"page"},{"location":"#Cheat-Sheet","page":"DataSplits","title":"Cheat Sheet","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Task Strategy Example\nMaximin split KennardStoneSplit split(X, KennardStoneSplit(0.8))\nJoint X–y split SPXYSplit split((X, y), SPXYSplit(0.7))\nCluster shuffle ClusterShuffleSplit split(X, ClusterShuffleSplit(clusters, 0.8))\nCluster stratified ClusterStratifiedSplit split(X, ClusterStratifiedSplit(clusters, :proportional; frac=0.7))\nTime-based split TimeSplit split(dates, TimeSplit(0.7))\nProperty-based split TargetPropertySplit split(y, TargetPropertyHigh(0.8))\nRandom split RandomSplit split(X, RandomSplit(0.7))\nRandomized Kennard Stone MoraisLimaMartinSplit split(X, MoraisLimaMartinSplit(0.8; swap_frac=0.1))","category":"page"},{"location":"#Supported-Strategies","page":"DataSplits","title":"Supported Strategies","text":"","category":"section"},{"location":"","page":"DataSplits","title":"DataSplits","text":"Strategy Purpose Complexity\nKennardStoneSplit Maximin split on X O(N²) time, O(N²) memory\nLazyKennardStoneSplit Same, streamed O(N²) time, O(N) mem\nSPXYSplit Joint X–y maximin (SPXY) O(N²) time, O(N²) mem\nMoraisLimaMartinSplit Kennard–Stone + random swap O(N²) time, O(N²) memory\nOptiSimSplit Optimisable dissimilarity-based splitting O(N²) time, O(N²) memory\nMinimumDissimilaritySplit Greedy dissimilarity with one candidate O(N²) time, O(N²) memory\nMaximumDissimilaritySplit Greedy dissimilarity with full pool O(N²) time, O(N²) memory\nClusterShuffleSplit Cluster-based shuffle split O(N²) time, O(N²) memory\nClusterStratifiedSplit Cluster-based stratified split (equal, proportional, Neyman). Selects a quota per cluster, then splits into train/test according to user fraction. O(N²) time, O(N²) memory","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"All splitting strategies in DataSplits are designed to work with any AbstractArray, including those with non-standard axes.","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"DataSplits expects data matrices to follow the Julia ML convention: columns are samples, rows are features. If your data uses rows as samples, transpose it before splitting (e.g., use X').","category":"page"},{"location":"","page":"DataSplits","title":"DataSplits","text":"For custom data types, implement Base.length (number of samples) and Base.getindex(data, i) (returning the i-th sample) as described in the MLUtils documentation. This ensures compatibility with all DataSplits algorithms and utilities.","category":"page"}]
}
